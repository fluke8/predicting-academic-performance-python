{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import  precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('csv/uspevaemost_nakop_bez_perevedennih_raw1.csv')\n",
    "# dataset = dataset[dataset['Квалификация'] != 3]\n",
    "# dataset = dataset[dataset['Квалификация'] != 4]\n",
    "data = dataset.iloc[:, :-1]\n",
    "target = dataset.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3)\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Семестр</th>\n",
       "      <th>Форма обучения</th>\n",
       "      <th>Квалификация</th>\n",
       "      <th>зачет сразу</th>\n",
       "      <th>удовлетворительно сразу</th>\n",
       "      <th>хорошо сразу</th>\n",
       "      <th>отлично сразу</th>\n",
       "      <th>зачет с исправлением</th>\n",
       "      <th>удовлетворительно с исправлением</th>\n",
       "      <th>хорошо с исправлением</th>\n",
       "      <th>...</th>\n",
       "      <th>Накоп удовлетворительно сразу</th>\n",
       "      <th>Накоп хорошо сразу</th>\n",
       "      <th>Накоп отлично сразу</th>\n",
       "      <th>Накоп зачет с исправлением</th>\n",
       "      <th>Накоп удовлетворительно с исправлением</th>\n",
       "      <th>Накоп хорошо с исправлением</th>\n",
       "      <th>Накоп отлично с исправлением</th>\n",
       "      <th>Накоп незачет до исправления</th>\n",
       "      <th>Накоп зачет до исправления</th>\n",
       "      <th>Накоп удовлетворительно до исправления</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Семестр  Форма обучения  Квалификация  зачет сразу  \\\n",
       "0        1               2             0          6.0   \n",
       "1        2               2             0          5.0   \n",
       "2        3               2             0          2.0   \n",
       "3        4               2             0          4.0   \n",
       "4        5               2             0          5.0   \n",
       "\n",
       "   удовлетворительно сразу  хорошо сразу  отлично сразу  зачет с исправлением  \\\n",
       "0                      0.0           1.0            1.0                   0.0   \n",
       "1                      1.0           0.0            2.0                   0.0   \n",
       "2                      0.0           2.0            1.0                   0.0   \n",
       "3                      0.0           0.0            2.0                   0.0   \n",
       "4                      0.0           2.0            0.0                   0.0   \n",
       "\n",
       "   удовлетворительно с исправлением  хорошо с исправлением  ...  \\\n",
       "0                               0.0                    0.0  ...   \n",
       "1                               0.0                    1.0  ...   \n",
       "2                               0.0                    1.0  ...   \n",
       "3                               0.0                    0.0  ...   \n",
       "4                               0.0                    1.0  ...   \n",
       "\n",
       "   Накоп удовлетворительно сразу  Накоп хорошо сразу  Накоп отлично сразу  \\\n",
       "0                            0.0                 1.0                  1.0   \n",
       "1                            1.0                 1.0                  3.0   \n",
       "2                            1.0                 3.0                  4.0   \n",
       "3                            1.0                 3.0                  6.0   \n",
       "4                            1.0                 5.0                  6.0   \n",
       "\n",
       "   Накоп зачет с исправлением  Накоп удовлетворительно с исправлением  \\\n",
       "0                         0.0                                     0.0   \n",
       "1                         0.0                                     0.0   \n",
       "2                         0.0                                     0.0   \n",
       "3                         0.0                                     0.0   \n",
       "4                         0.0                                     0.0   \n",
       "\n",
       "   Накоп хорошо с исправлением  Накоп отлично с исправлением  \\\n",
       "0                          0.0                           0.0   \n",
       "1                          1.0                           0.0   \n",
       "2                          2.0                           0.0   \n",
       "3                          2.0                           1.0   \n",
       "4                          3.0                           1.0   \n",
       "\n",
       "   Накоп незачет до исправления  Накоп зачет до исправления  \\\n",
       "0                           0.0                         0.0   \n",
       "1                           0.0                         0.0   \n",
       "2                           0.0                         0.0   \n",
       "3                           0.0                         0.0   \n",
       "4                           0.0                         0.0   \n",
       "\n",
       "   Накоп удовлетворительно до исправления  \n",
       "0                                     0.0  \n",
       "1                                     1.0  \n",
       "2                                     2.0  \n",
       "3                                     3.0  \n",
       "4                                     4.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Семестр</th>\n",
       "      <th>Форма обучения</th>\n",
       "      <th>Квалификация</th>\n",
       "      <th>незачет сразу</th>\n",
       "      <th>зачет сразу</th>\n",
       "      <th>удовлетворительно сразу</th>\n",
       "      <th>хорошо сразу</th>\n",
       "      <th>отлично сразу</th>\n",
       "      <th>зачет с исправлением</th>\n",
       "      <th>удовлетворительно с исправлением</th>\n",
       "      <th>...</th>\n",
       "      <th>Накоп удовлетворительно сразу</th>\n",
       "      <th>Накоп хорошо сразу</th>\n",
       "      <th>Накоп отлично сразу</th>\n",
       "      <th>Накоп зачет с исправлением</th>\n",
       "      <th>Накоп удовлетворительно с исправлением</th>\n",
       "      <th>Накоп хорошо с исправлением</th>\n",
       "      <th>Накоп отлично с исправлением</th>\n",
       "      <th>Накоп незачет до исправления</th>\n",
       "      <th>Накоп зачет до исправления</th>\n",
       "      <th>Накоп удовлетворительно до исправления</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20165.000000</td>\n",
       "      <td>20165.000000</td>\n",
       "      <td>20165.000000</td>\n",
       "      <td>20165.0</td>\n",
       "      <td>20165.000000</td>\n",
       "      <td>20165.000000</td>\n",
       "      <td>20165.000000</td>\n",
       "      <td>20165.000000</td>\n",
       "      <td>20165.000000</td>\n",
       "      <td>20165.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>20165.000000</td>\n",
       "      <td>20165.000000</td>\n",
       "      <td>20165.000000</td>\n",
       "      <td>20165.000000</td>\n",
       "      <td>20165.000000</td>\n",
       "      <td>20165.000000</td>\n",
       "      <td>20165.000000</td>\n",
       "      <td>20165.000000</td>\n",
       "      <td>20165.000000</td>\n",
       "      <td>20165.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.065807</td>\n",
       "      <td>0.581304</td>\n",
       "      <td>0.563452</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.950012</td>\n",
       "      <td>0.760476</td>\n",
       "      <td>1.253360</td>\n",
       "      <td>1.118324</td>\n",
       "      <td>0.004364</td>\n",
       "      <td>0.033028</td>\n",
       "      <td>...</td>\n",
       "      <td>4.176048</td>\n",
       "      <td>6.015522</td>\n",
       "      <td>4.274585</td>\n",
       "      <td>0.020035</td>\n",
       "      <td>0.151401</td>\n",
       "      <td>0.361914</td>\n",
       "      <td>0.468535</td>\n",
       "      <td>0.057377</td>\n",
       "      <td>0.411059</td>\n",
       "      <td>0.248153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.527685</td>\n",
       "      <td>0.904647</td>\n",
       "      <td>0.959507</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.630394</td>\n",
       "      <td>1.135160</td>\n",
       "      <td>1.209026</td>\n",
       "      <td>1.338356</td>\n",
       "      <td>0.098997</td>\n",
       "      <td>0.187647</td>\n",
       "      <td>...</td>\n",
       "      <td>6.208828</td>\n",
       "      <td>5.415883</td>\n",
       "      <td>5.308143</td>\n",
       "      <td>0.224460</td>\n",
       "      <td>0.512334</td>\n",
       "      <td>0.783900</td>\n",
       "      <td>0.913413</td>\n",
       "      <td>0.455554</td>\n",
       "      <td>0.815739</td>\n",
       "      <td>0.700421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Семестр  Форма обучения  Квалификация  незачет сразу  \\\n",
       "count  20165.000000    20165.000000  20165.000000        20165.0   \n",
       "mean       4.065807        0.581304      0.563452            0.0   \n",
       "std        2.527685        0.904647      0.959507            0.0   \n",
       "min        1.000000        0.000000      0.000000            0.0   \n",
       "25%        2.000000        0.000000      0.000000            0.0   \n",
       "50%        3.000000        0.000000      0.000000            0.0   \n",
       "75%        6.000000        2.000000      1.000000            0.0   \n",
       "max       11.000000        2.000000      3.000000            0.0   \n",
       "\n",
       "        зачет сразу  удовлетворительно сразу  хорошо сразу  отлично сразу  \\\n",
       "count  20165.000000             20165.000000  20165.000000   20165.000000   \n",
       "mean       3.950012                 0.760476      1.253360       1.118324   \n",
       "std        1.630394                 1.135160      1.209026       1.338356   \n",
       "min        0.000000                 0.000000      0.000000       0.000000   \n",
       "25%        3.000000                 0.000000      0.000000       0.000000   \n",
       "50%        4.000000                 0.000000      1.000000       1.000000   \n",
       "75%        5.000000                 1.000000      2.000000       2.000000   \n",
       "max       11.000000                 8.000000     11.000000      13.000000   \n",
       "\n",
       "       зачет с исправлением  удовлетворительно с исправлением  ...  \\\n",
       "count          20165.000000                      20165.000000  ...   \n",
       "mean               0.004364                          0.033028  ...   \n",
       "std                0.098997                          0.187647  ...   \n",
       "min                0.000000                          0.000000  ...   \n",
       "25%                0.000000                          0.000000  ...   \n",
       "50%                0.000000                          0.000000  ...   \n",
       "75%                0.000000                          0.000000  ...   \n",
       "max                5.000000                          3.000000  ...   \n",
       "\n",
       "       Накоп удовлетворительно сразу  Накоп хорошо сразу  Накоп отлично сразу  \\\n",
       "count                   20165.000000        20165.000000         20165.000000   \n",
       "mean                        4.176048            6.015522             4.274585   \n",
       "std                         6.208828            5.415883             5.308143   \n",
       "min                         0.000000            0.000000             0.000000   \n",
       "25%                         0.000000            2.000000             1.000000   \n",
       "50%                         1.000000            5.000000             2.000000   \n",
       "75%                         6.000000            9.000000             6.000000   \n",
       "max                        64.000000           38.000000            42.000000   \n",
       "\n",
       "       Накоп зачет с исправлением  Накоп удовлетворительно с исправлением  \\\n",
       "count                20165.000000                            20165.000000   \n",
       "mean                     0.020035                                0.151401   \n",
       "std                      0.224460                                0.512334   \n",
       "min                      0.000000                                0.000000   \n",
       "25%                      0.000000                                0.000000   \n",
       "50%                      0.000000                                0.000000   \n",
       "75%                      0.000000                                0.000000   \n",
       "max                     12.000000                                7.000000   \n",
       "\n",
       "       Накоп хорошо с исправлением  Накоп отлично с исправлением  \\\n",
       "count                 20165.000000                  20165.000000   \n",
       "mean                      0.361914                      0.468535   \n",
       "std                       0.783900                      0.913413   \n",
       "min                       0.000000                      0.000000   \n",
       "25%                       0.000000                      0.000000   \n",
       "50%                       0.000000                      0.000000   \n",
       "75%                       0.000000                      1.000000   \n",
       "max                       7.000000                      8.000000   \n",
       "\n",
       "       Накоп незачет до исправления  Накоп зачет до исправления  \\\n",
       "count                  20165.000000                20165.000000   \n",
       "mean                       0.057377                    0.411059   \n",
       "std                        0.455554                    0.815739   \n",
       "min                        0.000000                    0.000000   \n",
       "25%                        0.000000                    0.000000   \n",
       "50%                        0.000000                    0.000000   \n",
       "75%                        0.000000                    1.000000   \n",
       "max                       22.000000                    7.000000   \n",
       "\n",
       "       Накоп удовлетворительно до исправления  \n",
       "count                            20165.000000  \n",
       "mean                                 0.248153  \n",
       "std                                  0.700421  \n",
       "min                                  0.000000  \n",
       "25%                                  0.000000  \n",
       "50%                                  0.000000  \n",
       "75%                                  0.000000  \n",
       "max                                  6.000000  \n",
       "\n",
       "[8 rows x 28 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20165,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3341"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[target==1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16824,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[target == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DummyClassifier(strategy=&#x27;most_frequent&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DummyClassifier</label><div class=\"sk-toggleable__content\"><pre>DummyClassifier(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DummyClassifier(strategy='most_frequent')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DummyClassifier(strategy='most_frequent')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число ошибочно классифицированных образцов DummyClassifier:  999\n",
      "0.8348760330578512\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "0 - не отчислен       0.83      1.00      0.91      5051\n",
      "       Отчислен       0.00      0.00      0.00       999\n",
      "\n",
      "       accuracy                           0.83      6050\n",
      "      macro avg       0.42      0.50      0.46      6050\n",
      "   weighted avg       0.70      0.83      0.76      6050\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fluke\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\fluke\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\fluke\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(\"Число ошибочно классифицированных образцов DummyClassifier: % d\" % (y_test != y_pred).sum())\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=target_name))\n",
    "\n",
    "print()\n",
    "print('Логик регрессион')\n",
    "print('Вероятность моего отчисления за первый семестр:',clf.predict(pd.read_csv('uspevaemost_nakop_My.csv').iloc[:1,:])[:, 1] * 100)\n",
    "print('Вероятность моего отчисления за второй семестр:',clf.predict(pd.read_csv('uspevaemost_nakop_My.csv').iloc[1:2,:])[:, 1] * 100)\n",
    "print('Вероятность моего отчисления за третий семестр:',clf.predict(pd.read_csv('uspevaemost_nakop_My.csv').iloc[2:3,:])[:, 1] * 100)\n",
    "print('Вероятность моего отчисления за четвертый семестр:',clf.predict(pd.read_csv('uspevaemost_nakop_My.csv').iloc[3:4,:])[:, 1] * 100)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "0 - не отчислен       0.89      0.99      0.93      5019\n",
      "       Отчислен       0.88      0.37      0.53      1031\n",
      "\n",
      "       accuracy                           0.88      6050\n",
      "      macro avg       0.88      0.68      0.73      6050\n",
      "   weighted avg       0.88      0.88      0.86      6050\n",
      "\n",
      "0.8846280991735537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4966,   53],\n",
       "       [ 645,  386]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C=1000.0, max_iter=1000000000)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "target_name = ['0 - не отчислен', 'Отчислен']\n",
    "print(classification_report(y_test, y_pred, target_names=target_name))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print()\n",
    "print('Логик регрессион')\n",
    "print('Вероятность моего отчисления за первый семестр:',lr.predict_proba(pd.read_csv('csv/uspevaemost_nakop_My.csv').iloc[:1,:])[:, 1] * 100)\n",
    "print('Вероятность моего отчисления за второй семестр:',lr.predict_proba(pd.read_csv('csv/uspevaemost_nakop_My.csv').iloc[1:2,:])[:, 1] * 100)\n",
    "print('Вероятность моего отчисления за третий семестр:',lr.predict_proba(pd.read_csv('csv/uspevaemost_nakop_My.csv').iloc[2:3,:])[:, 1] * 100)\n",
    "print('Вероятность моего отчисления за четвертый семестр:',lr.predict_proba(pd.read_csv('csv/uspevaemost_nakop_My.csv').iloc[3:4,:])[:, 1] * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Глубина 1' 'Скорость обучения 0.1' '0.8505506030414264']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fluke\\Documents\\Code\\predicting-academic-performance-python\\02_tests.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fluke/Documents/Code/predicting-academic-performance-python/02_tests.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fluke/Documents/Code/predicting-academic-performance-python/02_tests.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     model \u001b[39m=\u001b[39m CatBoostClassifier(iterations\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, task_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGPU\u001b[39m\u001b[39m\"\u001b[39m, devices\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m0:1\u001b[39m\u001b[39m'\u001b[39m, depth\u001b[39m=\u001b[39mi,        \u001b[39m# Глубина деревьев\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fluke/Documents/Code/predicting-academic-performance-python/02_tests.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                             learning_rate\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m(\u001b[39m10\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mj),  \u001b[39m# Скорость обучения\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fluke/Documents/Code/predicting-academic-performance-python/02_tests.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                             loss_function\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLogloss\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m# Функция потерь для задачи классификации\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fluke/Documents/Code/predicting-academic-performance-python/02_tests.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                             logging_level\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSilent\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fluke/Documents/Code/predicting-academic-performance-python/02_tests.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     model\u001b[39m.\u001b[39;49mfit(X_train_std, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fluke/Documents/Code/predicting-academic-performance-python/02_tests.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test_std)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fluke/Documents/Code/predicting-academic-performance-python/02_tests.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     accuracy \u001b[39m=\u001b[39m accuracy_score(y_test, y_pred)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\catboost\\core.py:5100\u001b[0m, in \u001b[0;36mCatBoostClassifier.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5097\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m params:\n\u001b[0;32m   5098\u001b[0m     CatBoostClassifier\u001b[39m.\u001b[39m_check_is_compatible_loss(params[\u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m-> 5100\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, cat_features, text_features, embedding_features, \u001b[39mNone\u001b[39;49;00m, sample_weight, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, baseline, use_best_model,\n\u001b[0;32m   5101\u001b[0m           eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period,\n\u001b[0;32m   5102\u001b[0m           silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n\u001b[0;32m   5103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\catboost\\core.py:2319\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2315\u001b[0m allow_clear_pool \u001b[39m=\u001b[39m train_params[\u001b[39m\"\u001b[39m\u001b[39mallow_clear_pool\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   2317\u001b[0m \u001b[39mwith\u001b[39;00m log_fixup(log_cout, log_cerr), \\\n\u001b[0;32m   2318\u001b[0m     plot_wrapper(plot, plot_file, \u001b[39m'\u001b[39m\u001b[39mTraining plots\u001b[39m\u001b[39m'\u001b[39m, [_get_train_dir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params())]):\n\u001b[1;32m-> 2319\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(\n\u001b[0;32m   2320\u001b[0m         train_pool,\n\u001b[0;32m   2321\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39meval_sets\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m   2322\u001b[0m         params,\n\u001b[0;32m   2323\u001b[0m         allow_clear_pool,\n\u001b[0;32m   2324\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39minit_model\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[0;32m   2325\u001b[0m     )\n\u001b[0;32m   2327\u001b[0m \u001b[39m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[0;32m   2328\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_object\u001b[39m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\catboost\\core.py:1723\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1722\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_train\u001b[39m(\u001b[39mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[1;32m-> 1723\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_object\u001b[39m.\u001b[39;49m_train(train_pool, test_pool, params, allow_clear_pool, init_model\u001b[39m.\u001b[39;49m_object \u001b[39mif\u001b[39;49;00m init_model \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m   1724\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[1;32m_catboost.pyx:4645\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:4694\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best = np.zeros([3])\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1, 15):\n",
    "        for j in range(1, 3):\n",
    "            model = CatBoostClassifier(iterations=1000, task_type=\"GPU\", devices='0:1', depth=i,        # Глубина деревьев\n",
    "                                    learning_rate=1/(10**j),  # Скорость обучения\n",
    "                                    loss_function='Logloss',  # Функция потерь для задачи классификации\n",
    "                                    logging_level='Silent')\n",
    "            model.fit(X_train_std, y_train)\n",
    "\n",
    "            y_pred = model.predict(X_test_std)\n",
    "\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            report = classification_report(y_test, y_pred)\n",
    "\n",
    "            info = np.array([f'Глубина {i}', f'Скорость обучения {1/(10**j)}', accuracy])\n",
    "            print(info)\n",
    "            best = np.vstack((best, info))\n",
    "\n",
    "max_row_index = np.argmax(best[:, 2])\n",
    "\n",
    "# Extract the row with the maximum value\n",
    "max_row = best[max_row_index, :]\n",
    "\n",
    "print(\"Row with the maximum value in column {}: {}\".format(2, max_row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fluke\\Documents\\Code\\predicting-academic-performance-python\\02_tests.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fluke/Documents/Code/predicting-academic-performance-python/02_tests.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m CatBoostClassifier(iterations\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, task_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGPU\u001b[39m\u001b[39m\"\u001b[39m, devices\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m0:1\u001b[39m\u001b[39m'\u001b[39m, depth\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m,        \u001b[39m# Глубина деревьев\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fluke/Documents/Code/predicting-academic-performance-python/02_tests.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                                     learning_rate\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m,  \u001b[39m# Скорость обучения\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fluke/Documents/Code/predicting-academic-performance-python/02_tests.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                     loss_function\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLogloss\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m# Функция потерь для задачи классификации\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fluke/Documents/Code/predicting-academic-performance-python/02_tests.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                     logging_level\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSilent\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/fluke/Documents/Code/predicting-academic-performance-python/02_tests.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train_std, y_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fluke/Documents/Code/predicting-academic-performance-python/02_tests.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test_std)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fluke/Documents/Code/predicting-academic-performance-python/02_tests.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m accuracy \u001b[39m=\u001b[39m accuracy_score(y_test, y_pred)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\catboost\\core.py:5100\u001b[0m, in \u001b[0;36mCatBoostClassifier.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5097\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m params:\n\u001b[0;32m   5098\u001b[0m     CatBoostClassifier\u001b[39m.\u001b[39m_check_is_compatible_loss(params[\u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m-> 5100\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, cat_features, text_features, embedding_features, \u001b[39mNone\u001b[39;49;00m, sample_weight, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, baseline, use_best_model,\n\u001b[0;32m   5101\u001b[0m           eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period,\n\u001b[0;32m   5102\u001b[0m           silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n\u001b[0;32m   5103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\catboost\\core.py:2319\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2315\u001b[0m allow_clear_pool \u001b[39m=\u001b[39m train_params[\u001b[39m\"\u001b[39m\u001b[39mallow_clear_pool\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   2317\u001b[0m \u001b[39mwith\u001b[39;00m log_fixup(log_cout, log_cerr), \\\n\u001b[0;32m   2318\u001b[0m     plot_wrapper(plot, plot_file, \u001b[39m'\u001b[39m\u001b[39mTraining plots\u001b[39m\u001b[39m'\u001b[39m, [_get_train_dir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params())]):\n\u001b[1;32m-> 2319\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(\n\u001b[0;32m   2320\u001b[0m         train_pool,\n\u001b[0;32m   2321\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39meval_sets\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m   2322\u001b[0m         params,\n\u001b[0;32m   2323\u001b[0m         allow_clear_pool,\n\u001b[0;32m   2324\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39minit_model\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[0;32m   2325\u001b[0m     )\n\u001b[0;32m   2327\u001b[0m \u001b[39m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[0;32m   2328\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_object\u001b[39m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\catboost\\core.py:1723\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1722\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_train\u001b[39m(\u001b[39mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[1;32m-> 1723\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_object\u001b[39m.\u001b[39;49m_train(train_pool, test_pool, params, allow_clear_pool, init_model\u001b[39m.\u001b[39;49m_object \u001b[39mif\u001b[39;49;00m init_model \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m   1724\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[1;32m_catboost.pyx:4645\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:4694\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier(iterations=1000, task_type=\"GPU\", devices='0:1', depth=8,        # Глубина деревьев\n",
    "                                    learning_rate=0.1,  # Скорость обучения\n",
    "                                    loss_function='Logloss',  # Функция потерь для задачи классификации\n",
    "                                    logging_level='Silent')\n",
    "model.fit(X_train_std, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_std)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(accuracy)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4650,  116],\n",
       "       [ 588,  367]], dtype=int64)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "601"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_pred == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.8751966439433665\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93      4689\n",
      "           1       0.69      0.56      0.62      1032\n",
      "\n",
      "    accuracy                           0.88      5721\n",
      "   macro avg       0.80      0.75      0.77      5721\n",
      "weighted avg       0.87      0.88      0.87      5721\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4429,  260],\n",
       "       [ 454,  578]], dtype=int64)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_classifier = DecisionTreeClassifier()\n",
    "tree_classifier.fit(X_train_std, y_train)\n",
    "\n",
    "# Make predictions\n",
    "tree_predictions = tree_classifier.predict(X_test_std)\n",
    "\n",
    "# Evaluate the model\n",
    "tree_accuracy = accuracy_score(y_test, tree_predictions)\n",
    "print(f\"Decision Tree Accuracy: {tree_accuracy}\")\n",
    "\n",
    "report = classification_report(y_test,  tree_predictions)\n",
    "print(report)\n",
    "confusion_matrix(y_test, tree_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.8937248732739032\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.99      0.93      4689\n",
      "           1       0.90      0.42      0.58      1032\n",
      "\n",
      "    accuracy                           0.89      5721\n",
      "   macro avg       0.89      0.71      0.76      5721\n",
      "weighted avg       0.89      0.89      0.87      5721\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4638,   51],\n",
       "       [ 594,  438]], dtype=int64)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create and train a random forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "rf_predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy}\")\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.8835867855270058\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93      4689\n",
      "           1       0.96      0.37      0.54      1032\n",
      "\n",
      "    accuracy                           0.88      5721\n",
      "   macro avg       0.92      0.68      0.73      5721\n",
      "weighted avg       0.89      0.88      0.86      5721\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4671,   18],\n",
       "       [ 648,  384]], dtype=int64)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create and train an SVM classifier\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(X_train_std, y_train)\n",
    "\n",
    "# Make predictions\n",
    "svm_predictions = svm_classifier.predict(X_test_std)\n",
    "\n",
    "# Evaluate the model\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "print(f\"SVM Accuracy: {svm_accuracy}\")\n",
    "report = classification_report(y_test, svm_predictions)\n",
    "print(report)\n",
    "confusion_matrix(y_test, svm_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.8655829400454466\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.98      0.92      4689\n",
      "           1       0.80      0.34      0.48      1032\n",
      "\n",
      "    accuracy                           0.87      5721\n",
      "   macro avg       0.83      0.66      0.70      5721\n",
      "weighted avg       0.86      0.87      0.84      5721\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4600,   89],\n",
       "       [ 680,  352]], dtype=int64)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Create and train a Gaussian Naive Bayes classifier\n",
    "nb_classifier = GaussianNB()\n",
    "nb_classifier.fit(X_train_std, y_train)\n",
    "\n",
    "# Make predictions\n",
    "nb_predictions = nb_classifier.predict(X_test_std)\n",
    "\n",
    "# Evaluate the model\n",
    "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "print(f\"Naive Bayes Accuracy: {nb_accuracy}\")\n",
    "report = classification_report(y_test, nb_predictions)\n",
    "print(report)\n",
    "confusion_matrix(y_test, nb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 0.859465128474043\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.92      4689\n",
      "           1       0.63      0.53      0.58      1032\n",
      "\n",
      "    accuracy                           0.86      5721\n",
      "   macro avg       0.77      0.73      0.75      5721\n",
      "weighted avg       0.85      0.86      0.85      5721\n",
      "\n",
      "KNN Accuracy: 0.8706519839188953\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.98      0.93      4689\n",
      "           1       0.83      0.36      0.50      1032\n",
      "\n",
      "    accuracy                           0.87      5721\n",
      "   macro avg       0.85      0.67      0.71      5721\n",
      "weighted avg       0.87      0.87      0.85      5721\n",
      "\n",
      "KNN Accuracy: 0.8723999300821534\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93      4689\n",
      "           1       0.74      0.45      0.56      1032\n",
      "\n",
      "    accuracy                           0.87      5721\n",
      "   macro avg       0.81      0.71      0.74      5721\n",
      "weighted avg       0.86      0.87      0.86      5721\n",
      "\n",
      "KNN Accuracy: 0.8750218493270407\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.99      0.93      4689\n",
      "           1       0.87      0.36      0.51      1032\n",
      "\n",
      "    accuracy                           0.88      5721\n",
      "   macro avg       0.87      0.67      0.72      5721\n",
      "weighted avg       0.87      0.88      0.85      5721\n",
      "\n",
      "KNN Accuracy: 0.8753714385596923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93      4689\n",
      "           1       0.80      0.41      0.54      1032\n",
      "\n",
      "    accuracy                           0.88      5721\n",
      "   macro avg       0.84      0.69      0.74      5721\n",
      "weighted avg       0.87      0.88      0.86      5721\n",
      "\n",
      "KNN Accuracy: 0.8741478762454117\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.99      0.93      4689\n",
      "           1       0.89      0.34      0.50      1032\n",
      "\n",
      "    accuracy                           0.87      5721\n",
      "   macro avg       0.88      0.67      0.71      5721\n",
      "weighted avg       0.88      0.87      0.85      5721\n",
      "\n",
      "KNN Accuracy: 0.8748470547107149\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93      4689\n",
      "           1       0.84      0.38      0.52      1032\n",
      "\n",
      "    accuracy                           0.87      5721\n",
      "   macro avg       0.86      0.68      0.72      5721\n",
      "weighted avg       0.87      0.87      0.85      5721\n",
      "\n",
      "KNN Accuracy: 0.871875546233176\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.99      0.93      4689\n",
      "           1       0.90      0.33      0.48      1032\n",
      "\n",
      "    accuracy                           0.87      5721\n",
      "   macro avg       0.89      0.66      0.70      5721\n",
      "weighted avg       0.88      0.87      0.85      5721\n",
      "\n",
      "KNN Accuracy: 0.8717007516168502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.99      0.93      4689\n",
      "           1       0.84      0.36      0.50      1032\n",
      "\n",
      "    accuracy                           0.87      5721\n",
      "   macro avg       0.86      0.67      0.71      5721\n",
      "weighted avg       0.87      0.87      0.85      5721\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create and train a KNN classifier with k=5 (you can adjust k)\n",
    "for i in range(1, 10):\n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn_classifier.fit(X_train_std, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    knn_predictions = knn_classifier.predict(X_test_std)\n",
    "\n",
    "    # Evaluate the model\n",
    "    knn_accuracy = accuracy_score(y_test, knn_predictions)\n",
    "    print(f\"KNN Accuracy: {knn_accuracy}\")\n",
    "    report = classification_report(y_test, knn_predictions)\n",
    "    print(report)\n",
    "    confusion_matrix(y_test, knn_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row with the maximum value in column 2: ['Глубина 10' 'Скорость обучения 0.01' '0.9127774864534173']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [5 6 7]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create an initial NumPy array\n",
    "arr = np.array([1, 2, 3])\n",
    "\n",
    "# Append a single element\n",
    "\n",
    "\n",
    "# Append multiple elements (as a list)\n",
    "new_elements = [5, 6, 7]\n",
    "arr = np.vstack((arr, new_elements))\n",
    "\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2325, number of negative: 11023\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002899 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 380\n",
      "[LightGBM] [Info] Number of data points in the train set: 13348, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.174183 -> initscore=-1.556264\n",
      "[LightGBM] [Info] Start training from score -1.556264\n",
      "Accuracy: 0.9141758433840238\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95      4809\n",
      "           1       0.86      0.54      0.67       912\n",
      "\n",
      "    accuracy                           0.91      5721\n",
      "   macro avg       0.89      0.76      0.81      5721\n",
      "weighted avg       0.91      0.91      0.90      5721\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4713,   96],\n",
       "       [ 395,  517]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = lgb.Dataset(X_train_std, label=y_train)\n",
    "test_data = lgb.Dataset(X_test_std, label=y_test, reference=train_data)\n",
    "\n",
    "# Определяем параметры модели\n",
    "params = {\n",
    "    'objective': 'binary',  # для задачи бинарной классификации\n",
    "    'boosting_type': 'gbdt',\n",
    "    'metric': 'binary_logloss',  # метрика качества\n",
    "    'num_leaves': 50,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.9\n",
    "}\n",
    "\n",
    "# Обучаем модель\n",
    "num_round = 1000  # количество итераций обучения\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])\n",
    "\n",
    "# Предсказываем на тестовом наборе данных\n",
    "y_pred = bst.predict(X_test_std, num_iteration=bst.best_iteration)\n",
    "\n",
    "# Оцениваем качество модели\n",
    "y_pred_binary = np.round(y_pred)  # преобразуем вероятности в бинарные предсказания\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(report)\n",
    "confusion_matrix(y_test, y_pred_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-logloss:0.42092\ttrain-logloss:0.43862\n",
      "[1]\teval-logloss:0.40138\ttrain-logloss:0.41816\n",
      "[2]\teval-logloss:0.38580\ttrain-logloss:0.40161\n",
      "[3]\teval-logloss:0.37321\ttrain-logloss:0.38843\n",
      "[4]\teval-logloss:0.36260\ttrain-logloss:0.37748\n",
      "[5]\teval-logloss:0.35356\ttrain-logloss:0.36787\n",
      "[6]\teval-logloss:0.34593\ttrain-logloss:0.35966\n",
      "[7]\teval-logloss:0.33895\ttrain-logloss:0.35262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8]\teval-logloss:0.33272\ttrain-logloss:0.34624\n",
      "[9]\teval-logloss:0.32766\ttrain-logloss:0.34072\n",
      "[10]\teval-logloss:0.32286\ttrain-logloss:0.33590\n",
      "[11]\teval-logloss:0.31803\ttrain-logloss:0.33073\n",
      "[12]\teval-logloss:0.31362\ttrain-logloss:0.32619\n",
      "[13]\teval-logloss:0.30983\ttrain-logloss:0.32182\n",
      "[14]\teval-logloss:0.30636\ttrain-logloss:0.31852\n",
      "[15]\teval-logloss:0.30296\ttrain-logloss:0.31499\n",
      "[16]\teval-logloss:0.29984\ttrain-logloss:0.31190\n",
      "[17]\teval-logloss:0.29726\ttrain-logloss:0.30881\n",
      "[18]\teval-logloss:0.29519\ttrain-logloss:0.30644\n",
      "[19]\teval-logloss:0.29265\ttrain-logloss:0.30382\n",
      "[20]\teval-logloss:0.29057\ttrain-logloss:0.30187\n",
      "[21]\teval-logloss:0.28872\ttrain-logloss:0.29964\n",
      "[22]\teval-logloss:0.28695\ttrain-logloss:0.29764\n",
      "[23]\teval-logloss:0.28525\ttrain-logloss:0.29568\n",
      "[24]\teval-logloss:0.28358\ttrain-logloss:0.29374\n",
      "[25]\teval-logloss:0.28221\ttrain-logloss:0.29215\n",
      "[26]\teval-logloss:0.28100\ttrain-logloss:0.29066\n",
      "[27]\teval-logloss:0.27975\ttrain-logloss:0.28906\n",
      "[28]\teval-logloss:0.27804\ttrain-logloss:0.28688\n",
      "[29]\teval-logloss:0.27712\ttrain-logloss:0.28565\n",
      "[30]\teval-logloss:0.27640\ttrain-logloss:0.28463\n",
      "[31]\teval-logloss:0.27473\ttrain-logloss:0.28276\n",
      "[32]\teval-logloss:0.27352\ttrain-logloss:0.28143\n",
      "[33]\teval-logloss:0.27288\ttrain-logloss:0.28045\n",
      "[34]\teval-logloss:0.27235\ttrain-logloss:0.27965\n",
      "[35]\teval-logloss:0.27122\ttrain-logloss:0.27816\n",
      "[36]\teval-logloss:0.27044\ttrain-logloss:0.27743\n",
      "[37]\teval-logloss:0.27000\ttrain-logloss:0.27664\n",
      "[38]\teval-logloss:0.26870\ttrain-logloss:0.27505\n",
      "[39]\teval-logloss:0.26812\ttrain-logloss:0.27430\n",
      "[40]\teval-logloss:0.26704\ttrain-logloss:0.27324\n",
      "[41]\teval-logloss:0.26637\ttrain-logloss:0.27246\n",
      "[42]\teval-logloss:0.26538\ttrain-logloss:0.27138\n",
      "[43]\teval-logloss:0.26483\ttrain-logloss:0.27063\n",
      "[44]\teval-logloss:0.26442\ttrain-logloss:0.27003\n",
      "[45]\teval-logloss:0.26393\ttrain-logloss:0.26961\n",
      "[46]\teval-logloss:0.26313\ttrain-logloss:0.26865\n",
      "[47]\teval-logloss:0.26268\ttrain-logloss:0.26807\n",
      "[48]\teval-logloss:0.26205\ttrain-logloss:0.26737\n",
      "[49]\teval-logloss:0.26146\ttrain-logloss:0.26684\n",
      "[50]\teval-logloss:0.26108\ttrain-logloss:0.26636\n",
      "[51]\teval-logloss:0.26031\ttrain-logloss:0.26552\n",
      "[52]\teval-logloss:0.25967\ttrain-logloss:0.26480\n",
      "[53]\teval-logloss:0.25927\ttrain-logloss:0.26429\n",
      "[54]\teval-logloss:0.25875\ttrain-logloss:0.26370\n",
      "[55]\teval-logloss:0.25845\ttrain-logloss:0.26330\n",
      "[56]\teval-logloss:0.25796\ttrain-logloss:0.26270\n",
      "[57]\teval-logloss:0.25762\ttrain-logloss:0.26219\n",
      "[58]\teval-logloss:0.25731\ttrain-logloss:0.26184\n",
      "[59]\teval-logloss:0.25700\ttrain-logloss:0.26124\n",
      "[60]\teval-logloss:0.25656\ttrain-logloss:0.26070\n",
      "[61]\teval-logloss:0.25613\ttrain-logloss:0.26008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fluke\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "C:\\Users\\fluke\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:160: UserWarning: [21:08:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0cec3277c4d9d0165-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62]\teval-logloss:0.25572\ttrain-logloss:0.25952\n",
      "[63]\teval-logloss:0.25539\ttrain-logloss:0.25921\n",
      "[64]\teval-logloss:0.25515\ttrain-logloss:0.25897\n",
      "[65]\teval-logloss:0.25482\ttrain-logloss:0.25853\n",
      "[66]\teval-logloss:0.25445\ttrain-logloss:0.25802\n",
      "[67]\teval-logloss:0.25420\ttrain-logloss:0.25776\n",
      "[68]\teval-logloss:0.25403\ttrain-logloss:0.25748\n",
      "[69]\teval-logloss:0.25367\ttrain-logloss:0.25701\n",
      "[70]\teval-logloss:0.25348\ttrain-logloss:0.25682\n",
      "[71]\teval-logloss:0.25314\ttrain-logloss:0.25634\n",
      "[72]\teval-logloss:0.25267\ttrain-logloss:0.25585\n",
      "[73]\teval-logloss:0.25258\ttrain-logloss:0.25566\n",
      "[74]\teval-logloss:0.25235\ttrain-logloss:0.25538\n",
      "[75]\teval-logloss:0.25210\ttrain-logloss:0.25505\n",
      "[76]\teval-logloss:0.25167\ttrain-logloss:0.25449\n",
      "[77]\teval-logloss:0.25155\ttrain-logloss:0.25435\n",
      "[78]\teval-logloss:0.25146\ttrain-logloss:0.25412\n",
      "[79]\teval-logloss:0.25116\ttrain-logloss:0.25366\n",
      "[80]\teval-logloss:0.25081\ttrain-logloss:0.25304\n",
      "[81]\teval-logloss:0.25075\ttrain-logloss:0.25289\n",
      "[82]\teval-logloss:0.25055\ttrain-logloss:0.25249\n",
      "[83]\teval-logloss:0.25025\ttrain-logloss:0.25219\n",
      "[84]\teval-logloss:0.25011\ttrain-logloss:0.25205\n",
      "[85]\teval-logloss:0.24998\ttrain-logloss:0.25189\n",
      "[86]\teval-logloss:0.24968\ttrain-logloss:0.25149\n",
      "[87]\teval-logloss:0.24956\ttrain-logloss:0.25126\n",
      "[88]\teval-logloss:0.24950\ttrain-logloss:0.25109\n",
      "[89]\teval-logloss:0.24939\ttrain-logloss:0.25097\n",
      "[90]\teval-logloss:0.24917\ttrain-logloss:0.25061\n",
      "[91]\teval-logloss:0.24878\ttrain-logloss:0.25007\n",
      "[92]\teval-logloss:0.24870\ttrain-logloss:0.24996\n",
      "[93]\teval-logloss:0.24836\ttrain-logloss:0.24954\n",
      "[94]\teval-logloss:0.24824\ttrain-logloss:0.24914\n",
      "[95]\teval-logloss:0.24802\ttrain-logloss:0.24874\n",
      "[96]\teval-logloss:0.24792\ttrain-logloss:0.24863\n",
      "[97]\teval-logloss:0.24775\ttrain-logloss:0.24831\n",
      "[98]\teval-logloss:0.24744\ttrain-logloss:0.24792\n",
      "[99]\teval-logloss:0.24731\ttrain-logloss:0.24776\n",
      "[100]\teval-logloss:0.24717\ttrain-logloss:0.24749\n",
      "[101]\teval-logloss:0.24705\ttrain-logloss:0.24740\n",
      "[102]\teval-logloss:0.24700\ttrain-logloss:0.24727\n",
      "[103]\teval-logloss:0.24696\ttrain-logloss:0.24711\n",
      "[104]\teval-logloss:0.24688\ttrain-logloss:0.24694\n",
      "[105]\teval-logloss:0.24660\ttrain-logloss:0.24663\n",
      "[106]\teval-logloss:0.24636\ttrain-logloss:0.24636\n",
      "[107]\teval-logloss:0.24611\ttrain-logloss:0.24599\n",
      "[108]\teval-logloss:0.24591\ttrain-logloss:0.24571\n",
      "[109]\teval-logloss:0.24577\ttrain-logloss:0.24551\n",
      "[110]\teval-logloss:0.24571\ttrain-logloss:0.24542\n",
      "[111]\teval-logloss:0.24540\ttrain-logloss:0.24501\n",
      "[112]\teval-logloss:0.24519\ttrain-logloss:0.24464\n",
      "[113]\teval-logloss:0.24503\ttrain-logloss:0.24447\n",
      "[114]\teval-logloss:0.24488\ttrain-logloss:0.24434\n",
      "[115]\teval-logloss:0.24488\ttrain-logloss:0.24427\n",
      "[116]\teval-logloss:0.24486\ttrain-logloss:0.24421\n",
      "[117]\teval-logloss:0.24472\ttrain-logloss:0.24406\n",
      "[118]\teval-logloss:0.24465\ttrain-logloss:0.24376\n",
      "[119]\teval-logloss:0.24440\ttrain-logloss:0.24339\n",
      "[120]\teval-logloss:0.24427\ttrain-logloss:0.24325\n",
      "[121]\teval-logloss:0.24417\ttrain-logloss:0.24318\n",
      "[122]\teval-logloss:0.24409\ttrain-logloss:0.24305\n",
      "[123]\teval-logloss:0.24391\ttrain-logloss:0.24280\n",
      "[124]\teval-logloss:0.24378\ttrain-logloss:0.24267\n",
      "[125]\teval-logloss:0.24371\ttrain-logloss:0.24255\n",
      "[126]\teval-logloss:0.24359\ttrain-logloss:0.24228\n",
      "[127]\teval-logloss:0.24351\ttrain-logloss:0.24217\n",
      "[128]\teval-logloss:0.24332\ttrain-logloss:0.24186\n",
      "[129]\teval-logloss:0.24323\ttrain-logloss:0.24180\n",
      "[130]\teval-logloss:0.24325\ttrain-logloss:0.24174\n",
      "[131]\teval-logloss:0.24314\ttrain-logloss:0.24161\n",
      "[132]\teval-logloss:0.24298\ttrain-logloss:0.24146\n",
      "[133]\teval-logloss:0.24291\ttrain-logloss:0.24126\n",
      "[134]\teval-logloss:0.24286\ttrain-logloss:0.24118\n",
      "[135]\teval-logloss:0.24283\ttrain-logloss:0.24112\n",
      "[136]\teval-logloss:0.24263\ttrain-logloss:0.24086\n",
      "[137]\teval-logloss:0.24253\ttrain-logloss:0.24074\n",
      "[138]\teval-logloss:0.24235\ttrain-logloss:0.24041\n",
      "[139]\teval-logloss:0.24234\ttrain-logloss:0.24028\n",
      "[140]\teval-logloss:0.24226\ttrain-logloss:0.24012\n",
      "[141]\teval-logloss:0.24218\ttrain-logloss:0.24007\n",
      "[142]\teval-logloss:0.24213\ttrain-logloss:0.23994\n",
      "[143]\teval-logloss:0.24206\ttrain-logloss:0.23986\n",
      "[144]\teval-logloss:0.24185\ttrain-logloss:0.23959\n",
      "[145]\teval-logloss:0.24182\ttrain-logloss:0.23942\n",
      "[146]\teval-logloss:0.24176\ttrain-logloss:0.23921\n",
      "[147]\teval-logloss:0.24159\ttrain-logloss:0.23895\n",
      "[148]\teval-logloss:0.24148\ttrain-logloss:0.23873\n",
      "[149]\teval-logloss:0.24151\ttrain-logloss:0.23859\n",
      "[150]\teval-logloss:0.24136\ttrain-logloss:0.23835\n",
      "[151]\teval-logloss:0.24134\ttrain-logloss:0.23830\n",
      "[152]\teval-logloss:0.24123\ttrain-logloss:0.23818\n",
      "[153]\teval-logloss:0.24105\ttrain-logloss:0.23795\n",
      "[154]\teval-logloss:0.24101\ttrain-logloss:0.23785\n",
      "[155]\teval-logloss:0.24093\ttrain-logloss:0.23775\n",
      "[156]\teval-logloss:0.24089\ttrain-logloss:0.23765\n",
      "[157]\teval-logloss:0.24082\ttrain-logloss:0.23754\n",
      "[158]\teval-logloss:0.24078\ttrain-logloss:0.23737\n",
      "[159]\teval-logloss:0.24066\ttrain-logloss:0.23727\n",
      "[160]\teval-logloss:0.24062\ttrain-logloss:0.23708\n",
      "[161]\teval-logloss:0.24051\ttrain-logloss:0.23698\n",
      "[162]\teval-logloss:0.24038\ttrain-logloss:0.23679\n",
      "[163]\teval-logloss:0.24033\ttrain-logloss:0.23670\n",
      "[164]\teval-logloss:0.24024\ttrain-logloss:0.23661\n",
      "[165]\teval-logloss:0.24016\ttrain-logloss:0.23654\n",
      "[166]\teval-logloss:0.24007\ttrain-logloss:0.23629\n",
      "[167]\teval-logloss:0.24003\ttrain-logloss:0.23619\n",
      "[168]\teval-logloss:0.23995\ttrain-logloss:0.23612\n",
      "[169]\teval-logloss:0.23995\ttrain-logloss:0.23609\n",
      "[170]\teval-logloss:0.23991\ttrain-logloss:0.23602\n",
      "[171]\teval-logloss:0.23980\ttrain-logloss:0.23580\n",
      "[172]\teval-logloss:0.23961\ttrain-logloss:0.23560\n",
      "[173]\teval-logloss:0.23957\ttrain-logloss:0.23555\n",
      "[174]\teval-logloss:0.23952\ttrain-logloss:0.23547\n",
      "[175]\teval-logloss:0.23940\ttrain-logloss:0.23535\n",
      "[176]\teval-logloss:0.23933\ttrain-logloss:0.23513\n",
      "[177]\teval-logloss:0.23929\ttrain-logloss:0.23509\n",
      "[178]\teval-logloss:0.23928\ttrain-logloss:0.23491\n",
      "[179]\teval-logloss:0.23916\ttrain-logloss:0.23468\n",
      "[180]\teval-logloss:0.23907\ttrain-logloss:0.23448\n",
      "[181]\teval-logloss:0.23888\ttrain-logloss:0.23431\n",
      "[182]\teval-logloss:0.23879\ttrain-logloss:0.23414\n",
      "[183]\teval-logloss:0.23876\ttrain-logloss:0.23410\n",
      "[184]\teval-logloss:0.23867\ttrain-logloss:0.23387\n",
      "[185]\teval-logloss:0.23854\ttrain-logloss:0.23363\n",
      "[186]\teval-logloss:0.23853\ttrain-logloss:0.23356\n",
      "[187]\teval-logloss:0.23849\ttrain-logloss:0.23350\n",
      "[188]\teval-logloss:0.23838\ttrain-logloss:0.23331\n",
      "[189]\teval-logloss:0.23835\ttrain-logloss:0.23323\n",
      "[190]\teval-logloss:0.23828\ttrain-logloss:0.23316\n",
      "[191]\teval-logloss:0.23823\ttrain-logloss:0.23302\n",
      "[192]\teval-logloss:0.23810\ttrain-logloss:0.23285\n",
      "[193]\teval-logloss:0.23811\ttrain-logloss:0.23270\n",
      "[194]\teval-logloss:0.23799\ttrain-logloss:0.23255\n",
      "[195]\teval-logloss:0.23792\ttrain-logloss:0.23247\n",
      "[196]\teval-logloss:0.23791\ttrain-logloss:0.23242\n",
      "[197]\teval-logloss:0.23787\ttrain-logloss:0.23236\n",
      "[198]\teval-logloss:0.23791\ttrain-logloss:0.23228\n",
      "[199]\teval-logloss:0.23787\ttrain-logloss:0.23216\n",
      "[200]\teval-logloss:0.23786\ttrain-logloss:0.23206\n",
      "[201]\teval-logloss:0.23784\ttrain-logloss:0.23202\n",
      "[202]\teval-logloss:0.23778\ttrain-logloss:0.23196\n",
      "[203]\teval-logloss:0.23774\ttrain-logloss:0.23189\n",
      "[204]\teval-logloss:0.23758\ttrain-logloss:0.23162\n",
      "[205]\teval-logloss:0.23755\ttrain-logloss:0.23145\n",
      "[206]\teval-logloss:0.23749\ttrain-logloss:0.23127\n",
      "[207]\teval-logloss:0.23748\ttrain-logloss:0.23122\n",
      "[208]\teval-logloss:0.23748\ttrain-logloss:0.23118\n",
      "[209]\teval-logloss:0.23747\ttrain-logloss:0.23116\n",
      "[210]\teval-logloss:0.23736\ttrain-logloss:0.23103\n",
      "[211]\teval-logloss:0.23728\ttrain-logloss:0.23088\n",
      "[212]\teval-logloss:0.23715\ttrain-logloss:0.23073\n",
      "[213]\teval-logloss:0.23711\ttrain-logloss:0.23061\n",
      "[214]\teval-logloss:0.23706\ttrain-logloss:0.23056\n",
      "[215]\teval-logloss:0.23696\ttrain-logloss:0.23038\n",
      "[216]\teval-logloss:0.23694\ttrain-logloss:0.23023\n",
      "[217]\teval-logloss:0.23692\ttrain-logloss:0.23004\n",
      "[218]\teval-logloss:0.23691\ttrain-logloss:0.22999\n",
      "[219]\teval-logloss:0.23688\ttrain-logloss:0.22994\n",
      "[220]\teval-logloss:0.23682\ttrain-logloss:0.22979\n",
      "[221]\teval-logloss:0.23679\ttrain-logloss:0.22974\n",
      "[222]\teval-logloss:0.23678\ttrain-logloss:0.22972\n",
      "[223]\teval-logloss:0.23679\ttrain-logloss:0.22961\n",
      "[224]\teval-logloss:0.23673\ttrain-logloss:0.22943\n",
      "[225]\teval-logloss:0.23663\ttrain-logloss:0.22930\n",
      "[226]\teval-logloss:0.23652\ttrain-logloss:0.22920\n",
      "[227]\teval-logloss:0.23644\ttrain-logloss:0.22915\n",
      "[228]\teval-logloss:0.23645\ttrain-logloss:0.22901\n",
      "[229]\teval-logloss:0.23637\ttrain-logloss:0.22888\n",
      "[230]\teval-logloss:0.23630\ttrain-logloss:0.22884\n",
      "[231]\teval-logloss:0.23625\ttrain-logloss:0.22876\n",
      "[232]\teval-logloss:0.23623\ttrain-logloss:0.22870\n",
      "[233]\teval-logloss:0.23623\ttrain-logloss:0.22866\n",
      "[234]\teval-logloss:0.23626\ttrain-logloss:0.22862\n",
      "[235]\teval-logloss:0.23622\ttrain-logloss:0.22850\n",
      "[236]\teval-logloss:0.23623\ttrain-logloss:0.22837\n",
      "[237]\teval-logloss:0.23612\ttrain-logloss:0.22821\n",
      "[238]\teval-logloss:0.23602\ttrain-logloss:0.22805\n",
      "[239]\teval-logloss:0.23595\ttrain-logloss:0.22797\n",
      "[240]\teval-logloss:0.23581\ttrain-logloss:0.22781\n",
      "[241]\teval-logloss:0.23577\ttrain-logloss:0.22777\n",
      "[242]\teval-logloss:0.23575\ttrain-logloss:0.22772\n",
      "[243]\teval-logloss:0.23573\ttrain-logloss:0.22770\n",
      "[244]\teval-logloss:0.23574\ttrain-logloss:0.22766\n",
      "[245]\teval-logloss:0.23574\ttrain-logloss:0.22763\n",
      "[246]\teval-logloss:0.23567\ttrain-logloss:0.22755\n",
      "[247]\teval-logloss:0.23551\ttrain-logloss:0.22740\n",
      "[248]\teval-logloss:0.23547\ttrain-logloss:0.22726\n",
      "[249]\teval-logloss:0.23541\ttrain-logloss:0.22722\n",
      "[250]\teval-logloss:0.23540\ttrain-logloss:0.22709\n",
      "[251]\teval-logloss:0.23537\ttrain-logloss:0.22707\n",
      "[252]\teval-logloss:0.23521\ttrain-logloss:0.22690\n",
      "[253]\teval-logloss:0.23523\ttrain-logloss:0.22676\n",
      "[254]\teval-logloss:0.23522\ttrain-logloss:0.22670\n",
      "[255]\teval-logloss:0.23518\ttrain-logloss:0.22658\n",
      "[256]\teval-logloss:0.23514\ttrain-logloss:0.22653\n",
      "[257]\teval-logloss:0.23513\ttrain-logloss:0.22649\n",
      "[258]\teval-logloss:0.23515\ttrain-logloss:0.22636\n",
      "[259]\teval-logloss:0.23514\ttrain-logloss:0.22629\n",
      "[260]\teval-logloss:0.23512\ttrain-logloss:0.22620\n",
      "[261]\teval-logloss:0.23503\ttrain-logloss:0.22610\n",
      "[262]\teval-logloss:0.23502\ttrain-logloss:0.22608\n",
      "[263]\teval-logloss:0.23491\ttrain-logloss:0.22596\n",
      "[264]\teval-logloss:0.23482\ttrain-logloss:0.22577\n",
      "[265]\teval-logloss:0.23476\ttrain-logloss:0.22570\n",
      "[266]\teval-logloss:0.23471\ttrain-logloss:0.22559\n",
      "[267]\teval-logloss:0.23466\ttrain-logloss:0.22554\n",
      "[268]\teval-logloss:0.23467\ttrain-logloss:0.22551\n",
      "[269]\teval-logloss:0.23466\ttrain-logloss:0.22545\n",
      "[270]\teval-logloss:0.23457\ttrain-logloss:0.22532\n",
      "[271]\teval-logloss:0.23455\ttrain-logloss:0.22528\n",
      "[272]\teval-logloss:0.23452\ttrain-logloss:0.22524\n",
      "[273]\teval-logloss:0.23448\ttrain-logloss:0.22507\n",
      "[274]\teval-logloss:0.23448\ttrain-logloss:0.22504\n",
      "[275]\teval-logloss:0.23447\ttrain-logloss:0.22498\n",
      "[276]\teval-logloss:0.23440\ttrain-logloss:0.22493\n",
      "[277]\teval-logloss:0.23444\ttrain-logloss:0.22483\n",
      "[278]\teval-logloss:0.23442\ttrain-logloss:0.22474\n",
      "[279]\teval-logloss:0.23437\ttrain-logloss:0.22460\n",
      "[280]\teval-logloss:0.23435\ttrain-logloss:0.22459\n",
      "[281]\teval-logloss:0.23433\ttrain-logloss:0.22456\n",
      "[282]\teval-logloss:0.23423\ttrain-logloss:0.22444\n",
      "[283]\teval-logloss:0.23414\ttrain-logloss:0.22437\n",
      "[284]\teval-logloss:0.23421\ttrain-logloss:0.22426\n",
      "[285]\teval-logloss:0.23422\ttrain-logloss:0.22420\n",
      "[286]\teval-logloss:0.23421\ttrain-logloss:0.22410\n",
      "[287]\teval-logloss:0.23420\ttrain-logloss:0.22409\n",
      "[288]\teval-logloss:0.23421\ttrain-logloss:0.22406\n",
      "[289]\teval-logloss:0.23421\ttrain-logloss:0.22401\n",
      "[290]\teval-logloss:0.23423\ttrain-logloss:0.22388\n",
      "[291]\teval-logloss:0.23412\ttrain-logloss:0.22375\n",
      "[292]\teval-logloss:0.23413\ttrain-logloss:0.22364\n",
      "[293]\teval-logloss:0.23409\ttrain-logloss:0.22355\n",
      "[294]\teval-logloss:0.23410\ttrain-logloss:0.22353\n",
      "[295]\teval-logloss:0.23408\ttrain-logloss:0.22339\n",
      "[296]\teval-logloss:0.23407\ttrain-logloss:0.22327\n",
      "[297]\teval-logloss:0.23409\ttrain-logloss:0.22323\n",
      "[298]\teval-logloss:0.23403\ttrain-logloss:0.22314\n",
      "[299]\teval-logloss:0.23398\ttrain-logloss:0.22308\n",
      "[300]\teval-logloss:0.23399\ttrain-logloss:0.22295\n",
      "[301]\teval-logloss:0.23397\ttrain-logloss:0.22274\n",
      "[302]\teval-logloss:0.23400\ttrain-logloss:0.22265\n",
      "[303]\teval-logloss:0.23396\ttrain-logloss:0.22262\n",
      "[304]\teval-logloss:0.23397\ttrain-logloss:0.22251\n",
      "[305]\teval-logloss:0.23399\ttrain-logloss:0.22247\n",
      "[306]\teval-logloss:0.23399\ttrain-logloss:0.22244\n",
      "[307]\teval-logloss:0.23396\ttrain-logloss:0.22235\n",
      "[308]\teval-logloss:0.23387\ttrain-logloss:0.22229\n",
      "[309]\teval-logloss:0.23377\ttrain-logloss:0.22217\n",
      "[310]\teval-logloss:0.23375\ttrain-logloss:0.22203\n",
      "[311]\teval-logloss:0.23375\ttrain-logloss:0.22201\n",
      "[312]\teval-logloss:0.23374\ttrain-logloss:0.22193\n",
      "[313]\teval-logloss:0.23375\ttrain-logloss:0.22191\n",
      "[314]\teval-logloss:0.23374\ttrain-logloss:0.22187\n",
      "[315]\teval-logloss:0.23370\ttrain-logloss:0.22171\n",
      "[316]\teval-logloss:0.23368\ttrain-logloss:0.22169\n",
      "[317]\teval-logloss:0.23368\ttrain-logloss:0.22158\n",
      "[318]\teval-logloss:0.23367\ttrain-logloss:0.22148\n",
      "[319]\teval-logloss:0.23365\ttrain-logloss:0.22145\n",
      "[320]\teval-logloss:0.23363\ttrain-logloss:0.22143\n",
      "[321]\teval-logloss:0.23365\ttrain-logloss:0.22140\n",
      "[322]\teval-logloss:0.23362\ttrain-logloss:0.22137\n",
      "[323]\teval-logloss:0.23360\ttrain-logloss:0.22134\n",
      "[324]\teval-logloss:0.23358\ttrain-logloss:0.22122\n",
      "[325]\teval-logloss:0.23357\ttrain-logloss:0.22121\n",
      "[326]\teval-logloss:0.23353\ttrain-logloss:0.22106\n",
      "[327]\teval-logloss:0.23360\ttrain-logloss:0.22098\n",
      "[328]\teval-logloss:0.23361\ttrain-logloss:0.22093\n",
      "[329]\teval-logloss:0.23349\ttrain-logloss:0.22086\n",
      "[330]\teval-logloss:0.23353\ttrain-logloss:0.22079\n",
      "[331]\teval-logloss:0.23349\ttrain-logloss:0.22072\n",
      "[332]\teval-logloss:0.23349\ttrain-logloss:0.22062\n",
      "[333]\teval-logloss:0.23350\ttrain-logloss:0.22059\n",
      "[334]\teval-logloss:0.23353\ttrain-logloss:0.22056\n",
      "[335]\teval-logloss:0.23359\ttrain-logloss:0.22046\n",
      "[336]\teval-logloss:0.23358\ttrain-logloss:0.22040\n",
      "[337]\teval-logloss:0.23359\ttrain-logloss:0.22038\n",
      "[338]\teval-logloss:0.23359\ttrain-logloss:0.22021\n",
      "[339]\teval-logloss:0.23356\ttrain-logloss:0.22018\n",
      "[340]\teval-logloss:0.23346\ttrain-logloss:0.22007\n",
      "[341]\teval-logloss:0.23342\ttrain-logloss:0.21997\n",
      "[342]\teval-logloss:0.23338\ttrain-logloss:0.21994\n",
      "[343]\teval-logloss:0.23339\ttrain-logloss:0.21982\n",
      "[344]\teval-logloss:0.23338\ttrain-logloss:0.21977\n",
      "[345]\teval-logloss:0.23344\ttrain-logloss:0.21968\n",
      "[346]\teval-logloss:0.23336\ttrain-logloss:0.21959\n",
      "[347]\teval-logloss:0.23333\ttrain-logloss:0.21956\n",
      "[348]\teval-logloss:0.23321\ttrain-logloss:0.21946\n",
      "[349]\teval-logloss:0.23324\ttrain-logloss:0.21937\n",
      "[350]\teval-logloss:0.23325\ttrain-logloss:0.21935\n",
      "[351]\teval-logloss:0.23315\ttrain-logloss:0.21927\n",
      "[352]\teval-logloss:0.23313\ttrain-logloss:0.21917\n",
      "[353]\teval-logloss:0.23312\ttrain-logloss:0.21915\n",
      "[354]\teval-logloss:0.23308\ttrain-logloss:0.21906\n",
      "[355]\teval-logloss:0.23308\ttrain-logloss:0.21898\n",
      "[356]\teval-logloss:0.23302\ttrain-logloss:0.21888\n",
      "[357]\teval-logloss:0.23302\ttrain-logloss:0.21877\n",
      "[358]\teval-logloss:0.23304\ttrain-logloss:0.21870\n",
      "[359]\teval-logloss:0.23303\ttrain-logloss:0.21869\n",
      "[360]\teval-logloss:0.23299\ttrain-logloss:0.21867\n",
      "[361]\teval-logloss:0.23303\ttrain-logloss:0.21862\n",
      "[362]\teval-logloss:0.23291\ttrain-logloss:0.21845\n",
      "[363]\teval-logloss:0.23287\ttrain-logloss:0.21840\n",
      "[364]\teval-logloss:0.23289\ttrain-logloss:0.21838\n",
      "[365]\teval-logloss:0.23290\ttrain-logloss:0.21835\n",
      "[366]\teval-logloss:0.23290\ttrain-logloss:0.21833\n",
      "[367]\teval-logloss:0.23285\ttrain-logloss:0.21814\n",
      "[368]\teval-logloss:0.23280\ttrain-logloss:0.21801\n",
      "[369]\teval-logloss:0.23279\ttrain-logloss:0.21798\n",
      "[370]\teval-logloss:0.23274\ttrain-logloss:0.21788\n",
      "[371]\teval-logloss:0.23272\ttrain-logloss:0.21776\n",
      "[372]\teval-logloss:0.23270\ttrain-logloss:0.21763\n",
      "[373]\teval-logloss:0.23270\ttrain-logloss:0.21756\n",
      "[374]\teval-logloss:0.23268\ttrain-logloss:0.21747\n",
      "[375]\teval-logloss:0.23268\ttrain-logloss:0.21746\n",
      "[376]\teval-logloss:0.23265\ttrain-logloss:0.21738\n",
      "[377]\teval-logloss:0.23267\ttrain-logloss:0.21731\n",
      "[378]\teval-logloss:0.23258\ttrain-logloss:0.21724\n",
      "[379]\teval-logloss:0.23251\ttrain-logloss:0.21711\n",
      "[380]\teval-logloss:0.23245\ttrain-logloss:0.21704\n",
      "[381]\teval-logloss:0.23246\ttrain-logloss:0.21704\n",
      "[382]\teval-logloss:0.23246\ttrain-logloss:0.21690\n",
      "[383]\teval-logloss:0.23242\ttrain-logloss:0.21683\n",
      "[384]\teval-logloss:0.23247\ttrain-logloss:0.21677\n",
      "[385]\teval-logloss:0.23248\ttrain-logloss:0.21664\n",
      "[386]\teval-logloss:0.23247\ttrain-logloss:0.21662\n",
      "[387]\teval-logloss:0.23249\ttrain-logloss:0.21650\n",
      "[388]\teval-logloss:0.23249\ttrain-logloss:0.21644\n",
      "[389]\teval-logloss:0.23248\ttrain-logloss:0.21643\n",
      "[390]\teval-logloss:0.23249\ttrain-logloss:0.21641\n",
      "[391]\teval-logloss:0.23250\ttrain-logloss:0.21639\n",
      "[392]\teval-logloss:0.23249\ttrain-logloss:0.21628\n",
      "[393]\teval-logloss:0.23252\ttrain-logloss:0.21621\n",
      "[394]\teval-logloss:0.23253\ttrain-logloss:0.21613\n",
      "[395]\teval-logloss:0.23251\ttrain-logloss:0.21611\n",
      "[396]\teval-logloss:0.23251\ttrain-logloss:0.21603\n",
      "[397]\teval-logloss:0.23255\ttrain-logloss:0.21595\n",
      "[398]\teval-logloss:0.23254\ttrain-logloss:0.21591\n",
      "[399]\teval-logloss:0.23253\ttrain-logloss:0.21589\n",
      "[400]\teval-logloss:0.23243\ttrain-logloss:0.21580\n",
      "[401]\teval-logloss:0.23244\ttrain-logloss:0.21571\n",
      "[402]\teval-logloss:0.23242\ttrain-logloss:0.21563\n",
      "[403]\teval-logloss:0.23241\ttrain-logloss:0.21561\n",
      "[404]\teval-logloss:0.23243\ttrain-logloss:0.21559\n",
      "[405]\teval-logloss:0.23239\ttrain-logloss:0.21548\n",
      "[406]\teval-logloss:0.23229\ttrain-logloss:0.21534\n",
      "[407]\teval-logloss:0.23233\ttrain-logloss:0.21527\n",
      "[408]\teval-logloss:0.23233\ttrain-logloss:0.21525\n",
      "[409]\teval-logloss:0.23233\ttrain-logloss:0.21523\n",
      "[410]\teval-logloss:0.23229\ttrain-logloss:0.21515\n",
      "[411]\teval-logloss:0.23231\ttrain-logloss:0.21504\n",
      "[412]\teval-logloss:0.23231\ttrain-logloss:0.21497\n",
      "[413]\teval-logloss:0.23231\ttrain-logloss:0.21493\n",
      "[414]\teval-logloss:0.23230\ttrain-logloss:0.21488\n",
      "[415]\teval-logloss:0.23232\ttrain-logloss:0.21487\n",
      "[416]\teval-logloss:0.23231\ttrain-logloss:0.21485\n",
      "[417]\teval-logloss:0.23230\ttrain-logloss:0.21472\n",
      "[418]\teval-logloss:0.23231\ttrain-logloss:0.21470\n",
      "[419]\teval-logloss:0.23232\ttrain-logloss:0.21467\n",
      "[420]\teval-logloss:0.23226\ttrain-logloss:0.21454\n",
      "[421]\teval-logloss:0.23223\ttrain-logloss:0.21448\n",
      "[422]\teval-logloss:0.23212\ttrain-logloss:0.21434\n",
      "[423]\teval-logloss:0.23209\ttrain-logloss:0.21431\n",
      "[424]\teval-logloss:0.23205\ttrain-logloss:0.21428\n",
      "[425]\teval-logloss:0.23205\ttrain-logloss:0.21425\n",
      "[426]\teval-logloss:0.23208\ttrain-logloss:0.21418\n",
      "[427]\teval-logloss:0.23205\ttrain-logloss:0.21407\n",
      "[428]\teval-logloss:0.23199\ttrain-logloss:0.21402\n",
      "[429]\teval-logloss:0.23202\ttrain-logloss:0.21395\n",
      "[430]\teval-logloss:0.23202\ttrain-logloss:0.21393\n",
      "[431]\teval-logloss:0.23201\ttrain-logloss:0.21392\n",
      "[432]\teval-logloss:0.23200\ttrain-logloss:0.21391\n",
      "[433]\teval-logloss:0.23200\ttrain-logloss:0.21390\n",
      "[434]\teval-logloss:0.23198\ttrain-logloss:0.21387\n",
      "[435]\teval-logloss:0.23192\ttrain-logloss:0.21377\n",
      "[436]\teval-logloss:0.23177\ttrain-logloss:0.21366\n",
      "[437]\teval-logloss:0.23179\ttrain-logloss:0.21359\n",
      "[438]\teval-logloss:0.23176\ttrain-logloss:0.21356\n",
      "[439]\teval-logloss:0.23175\ttrain-logloss:0.21350\n",
      "[440]\teval-logloss:0.23175\ttrain-logloss:0.21343\n",
      "[441]\teval-logloss:0.23178\ttrain-logloss:0.21339\n",
      "[442]\teval-logloss:0.23176\ttrain-logloss:0.21334\n",
      "[443]\teval-logloss:0.23175\ttrain-logloss:0.21332\n",
      "[444]\teval-logloss:0.23180\ttrain-logloss:0.21327\n",
      "[445]\teval-logloss:0.23179\ttrain-logloss:0.21321\n",
      "[446]\teval-logloss:0.23178\ttrain-logloss:0.21320\n",
      "[447]\teval-logloss:0.23173\ttrain-logloss:0.21311\n",
      "[448]\teval-logloss:0.23173\ttrain-logloss:0.21310\n",
      "[449]\teval-logloss:0.23176\ttrain-logloss:0.21298\n",
      "[450]\teval-logloss:0.23174\ttrain-logloss:0.21290\n",
      "[451]\teval-logloss:0.23173\ttrain-logloss:0.21289\n",
      "[452]\teval-logloss:0.23172\ttrain-logloss:0.21281\n",
      "[453]\teval-logloss:0.23178\ttrain-logloss:0.21273\n",
      "[454]\teval-logloss:0.23182\ttrain-logloss:0.21271\n",
      "[455]\teval-logloss:0.23178\ttrain-logloss:0.21261\n",
      "[456]\teval-logloss:0.23177\ttrain-logloss:0.21257\n",
      "[457]\teval-logloss:0.23175\ttrain-logloss:0.21251\n",
      "[458]\teval-logloss:0.23172\ttrain-logloss:0.21245\n",
      "[459]\teval-logloss:0.23169\ttrain-logloss:0.21238\n",
      "[460]\teval-logloss:0.23163\ttrain-logloss:0.21231\n",
      "[461]\teval-logloss:0.23170\ttrain-logloss:0.21225\n",
      "[462]\teval-logloss:0.23167\ttrain-logloss:0.21216\n",
      "[463]\teval-logloss:0.23159\ttrain-logloss:0.21208\n",
      "[464]\teval-logloss:0.23158\ttrain-logloss:0.21207\n",
      "[465]\teval-logloss:0.23160\ttrain-logloss:0.21201\n",
      "[466]\teval-logloss:0.23160\ttrain-logloss:0.21198\n",
      "[467]\teval-logloss:0.23156\ttrain-logloss:0.21196\n",
      "[468]\teval-logloss:0.23155\ttrain-logloss:0.21194\n",
      "[469]\teval-logloss:0.23156\ttrain-logloss:0.21190\n",
      "[470]\teval-logloss:0.23156\ttrain-logloss:0.21184\n",
      "[471]\teval-logloss:0.23157\ttrain-logloss:0.21181\n",
      "[472]\teval-logloss:0.23157\ttrain-logloss:0.21174\n",
      "[473]\teval-logloss:0.23156\ttrain-logloss:0.21169\n",
      "[474]\teval-logloss:0.23154\ttrain-logloss:0.21166\n",
      "[475]\teval-logloss:0.23164\ttrain-logloss:0.21153\n",
      "[476]\teval-logloss:0.23159\ttrain-logloss:0.21146\n",
      "[477]\teval-logloss:0.23157\ttrain-logloss:0.21144\n",
      "[478]\teval-logloss:0.23158\ttrain-logloss:0.21140\n",
      "[479]\teval-logloss:0.23160\ttrain-logloss:0.21133\n",
      "[480]\teval-logloss:0.23161\ttrain-logloss:0.21131\n",
      "[481]\teval-logloss:0.23159\ttrain-logloss:0.21125\n",
      "[482]\teval-logloss:0.23159\ttrain-logloss:0.21124\n",
      "[483]\teval-logloss:0.23158\ttrain-logloss:0.21121\n",
      "[484]\teval-logloss:0.23160\ttrain-logloss:0.21120\n",
      "[485]\teval-logloss:0.23162\ttrain-logloss:0.21118\n",
      "[486]\teval-logloss:0.23156\ttrain-logloss:0.21113\n",
      "[487]\teval-logloss:0.23156\ttrain-logloss:0.21110\n",
      "[488]\teval-logloss:0.23155\ttrain-logloss:0.21105\n",
      "[489]\teval-logloss:0.23156\ttrain-logloss:0.21097\n",
      "[490]\teval-logloss:0.23154\ttrain-logloss:0.21096\n",
      "[491]\teval-logloss:0.23145\ttrain-logloss:0.21086\n",
      "[492]\teval-logloss:0.23147\ttrain-logloss:0.21084\n",
      "[493]\teval-logloss:0.23147\ttrain-logloss:0.21083\n",
      "[494]\teval-logloss:0.23145\ttrain-logloss:0.21082\n",
      "[495]\teval-logloss:0.23142\ttrain-logloss:0.21075\n",
      "[496]\teval-logloss:0.23142\ttrain-logloss:0.21069\n",
      "[497]\teval-logloss:0.23145\ttrain-logloss:0.21064\n",
      "[498]\teval-logloss:0.23149\ttrain-logloss:0.21057\n",
      "[499]\teval-logloss:0.23148\ttrain-logloss:0.21056\n",
      "[500]\teval-logloss:0.23146\ttrain-logloss:0.21055\n",
      "[501]\teval-logloss:0.23146\ttrain-logloss:0.21053\n",
      "[502]\teval-logloss:0.23143\ttrain-logloss:0.21045\n",
      "[503]\teval-logloss:0.23137\ttrain-logloss:0.21036\n",
      "[504]\teval-logloss:0.23126\ttrain-logloss:0.21025\n",
      "[505]\teval-logloss:0.23128\ttrain-logloss:0.21019\n",
      "[506]\teval-logloss:0.23128\ttrain-logloss:0.21016\n",
      "[507]\teval-logloss:0.23120\ttrain-logloss:0.21004\n",
      "[508]\teval-logloss:0.23114\ttrain-logloss:0.20996\n",
      "[509]\teval-logloss:0.23116\ttrain-logloss:0.20987\n",
      "[510]\teval-logloss:0.23113\ttrain-logloss:0.20980\n",
      "[511]\teval-logloss:0.23110\ttrain-logloss:0.20973\n",
      "[512]\teval-logloss:0.23111\ttrain-logloss:0.20971\n",
      "[513]\teval-logloss:0.23109\ttrain-logloss:0.20968\n",
      "[514]\teval-logloss:0.23107\ttrain-logloss:0.20961\n",
      "[515]\teval-logloss:0.23105\ttrain-logloss:0.20958\n",
      "[516]\teval-logloss:0.23103\ttrain-logloss:0.20956\n",
      "[517]\teval-logloss:0.23112\ttrain-logloss:0.20949\n",
      "[518]\teval-logloss:0.23108\ttrain-logloss:0.20942\n",
      "[519]\teval-logloss:0.23101\ttrain-logloss:0.20932\n",
      "[520]\teval-logloss:0.23103\ttrain-logloss:0.20931\n",
      "[521]\teval-logloss:0.23100\ttrain-logloss:0.20925\n",
      "[522]\teval-logloss:0.23097\ttrain-logloss:0.20918\n",
      "[523]\teval-logloss:0.23100\ttrain-logloss:0.20913\n",
      "[524]\teval-logloss:0.23098\ttrain-logloss:0.20912\n",
      "[525]\teval-logloss:0.23098\ttrain-logloss:0.20911\n",
      "[526]\teval-logloss:0.23085\ttrain-logloss:0.20903\n",
      "[527]\teval-logloss:0.23085\ttrain-logloss:0.20897\n",
      "[528]\teval-logloss:0.23088\ttrain-logloss:0.20894\n",
      "[529]\teval-logloss:0.23086\ttrain-logloss:0.20891\n",
      "[530]\teval-logloss:0.23087\ttrain-logloss:0.20887\n",
      "[531]\teval-logloss:0.23086\ttrain-logloss:0.20885\n",
      "[532]\teval-logloss:0.23086\ttrain-logloss:0.20884\n",
      "[533]\teval-logloss:0.23088\ttrain-logloss:0.20881\n",
      "[534]\teval-logloss:0.23084\ttrain-logloss:0.20871\n",
      "[535]\teval-logloss:0.23090\ttrain-logloss:0.20866\n",
      "[536]\teval-logloss:0.23090\ttrain-logloss:0.20866\n",
      "[537]\teval-logloss:0.23087\ttrain-logloss:0.20864\n",
      "[538]\teval-logloss:0.23086\ttrain-logloss:0.20863\n",
      "[539]\teval-logloss:0.23088\ttrain-logloss:0.20861\n",
      "[540]\teval-logloss:0.23089\ttrain-logloss:0.20859\n",
      "[541]\teval-logloss:0.23089\ttrain-logloss:0.20858\n",
      "[542]\teval-logloss:0.23087\ttrain-logloss:0.20856\n",
      "[543]\teval-logloss:0.23086\ttrain-logloss:0.20851\n",
      "[544]\teval-logloss:0.23087\ttrain-logloss:0.20851\n",
      "[545]\teval-logloss:0.23088\ttrain-logloss:0.20849\n",
      "[546]\teval-logloss:0.23088\ttrain-logloss:0.20848\n",
      "[547]\teval-logloss:0.23088\ttrain-logloss:0.20844\n",
      "[548]\teval-logloss:0.23085\ttrain-logloss:0.20837\n",
      "[549]\teval-logloss:0.23081\ttrain-logloss:0.20829\n",
      "[550]\teval-logloss:0.23081\ttrain-logloss:0.20824\n",
      "[551]\teval-logloss:0.23079\ttrain-logloss:0.20817\n",
      "[552]\teval-logloss:0.23081\ttrain-logloss:0.20814\n",
      "[553]\teval-logloss:0.23080\ttrain-logloss:0.20804\n",
      "[554]\teval-logloss:0.23082\ttrain-logloss:0.20795\n",
      "[555]\teval-logloss:0.23084\ttrain-logloss:0.20785\n",
      "[556]\teval-logloss:0.23079\ttrain-logloss:0.20778\n",
      "[557]\teval-logloss:0.23078\ttrain-logloss:0.20777\n",
      "[558]\teval-logloss:0.23082\ttrain-logloss:0.20775\n",
      "[559]\teval-logloss:0.23079\ttrain-logloss:0.20768\n",
      "[560]\teval-logloss:0.23079\ttrain-logloss:0.20763\n",
      "[561]\teval-logloss:0.23083\ttrain-logloss:0.20754\n",
      "[562]\teval-logloss:0.23087\ttrain-logloss:0.20748\n",
      "[563]\teval-logloss:0.23087\ttrain-logloss:0.20747\n",
      "[564]\teval-logloss:0.23084\ttrain-logloss:0.20741\n",
      "[565]\teval-logloss:0.23082\ttrain-logloss:0.20735\n",
      "[566]\teval-logloss:0.23082\ttrain-logloss:0.20730\n",
      "[567]\teval-logloss:0.23081\ttrain-logloss:0.20729\n",
      "[568]\teval-logloss:0.23081\ttrain-logloss:0.20725\n",
      "[569]\teval-logloss:0.23081\ttrain-logloss:0.20722\n",
      "[570]\teval-logloss:0.23077\ttrain-logloss:0.20716\n",
      "[571]\teval-logloss:0.23077\ttrain-logloss:0.20711\n",
      "[572]\teval-logloss:0.23077\ttrain-logloss:0.20707\n",
      "[573]\teval-logloss:0.23078\ttrain-logloss:0.20706\n",
      "[574]\teval-logloss:0.23080\ttrain-logloss:0.20702\n",
      "[575]\teval-logloss:0.23077\ttrain-logloss:0.20696\n",
      "[576]\teval-logloss:0.23072\ttrain-logloss:0.20688\n",
      "[577]\teval-logloss:0.23075\ttrain-logloss:0.20684\n",
      "[578]\teval-logloss:0.23076\ttrain-logloss:0.20683\n",
      "[579]\teval-logloss:0.23077\ttrain-logloss:0.20678\n",
      "[580]\teval-logloss:0.23078\ttrain-logloss:0.20678\n",
      "[581]\teval-logloss:0.23081\ttrain-logloss:0.20672\n",
      "[582]\teval-logloss:0.23082\ttrain-logloss:0.20669\n",
      "[583]\teval-logloss:0.23082\ttrain-logloss:0.20667\n",
      "[584]\teval-logloss:0.23082\ttrain-logloss:0.20658\n",
      "[585]\teval-logloss:0.23081\ttrain-logloss:0.20653\n",
      "[586]\teval-logloss:0.23081\ttrain-logloss:0.20651\n",
      "[587]\teval-logloss:0.23081\ttrain-logloss:0.20647\n",
      "[588]\teval-logloss:0.23085\ttrain-logloss:0.20645\n",
      "[589]\teval-logloss:0.23092\ttrain-logloss:0.20638\n",
      "[590]\teval-logloss:0.23093\ttrain-logloss:0.20636\n",
      "[591]\teval-logloss:0.23095\ttrain-logloss:0.20634\n",
      "[592]\teval-logloss:0.23090\ttrain-logloss:0.20630\n",
      "[593]\teval-logloss:0.23092\ttrain-logloss:0.20624\n",
      "[594]\teval-logloss:0.23088\ttrain-logloss:0.20617\n",
      "[595]\teval-logloss:0.23087\ttrain-logloss:0.20609\n",
      "[596]\teval-logloss:0.23088\ttrain-logloss:0.20605\n",
      "[597]\teval-logloss:0.23089\ttrain-logloss:0.20603\n",
      "[598]\teval-logloss:0.23087\ttrain-logloss:0.20601\n",
      "[599]\teval-logloss:0.23086\ttrain-logloss:0.20599\n",
      "[600]\teval-logloss:0.23085\ttrain-logloss:0.20593\n",
      "[601]\teval-logloss:0.23082\ttrain-logloss:0.20589\n",
      "[602]\teval-logloss:0.23081\ttrain-logloss:0.20585\n",
      "[603]\teval-logloss:0.23079\ttrain-logloss:0.20582\n",
      "[604]\teval-logloss:0.23079\ttrain-logloss:0.20575\n",
      "[605]\teval-logloss:0.23075\ttrain-logloss:0.20568\n",
      "[606]\teval-logloss:0.23075\ttrain-logloss:0.20559\n",
      "[607]\teval-logloss:0.23073\ttrain-logloss:0.20558\n",
      "[608]\teval-logloss:0.23075\ttrain-logloss:0.20553\n",
      "[609]\teval-logloss:0.23075\ttrain-logloss:0.20552\n",
      "[610]\teval-logloss:0.23068\ttrain-logloss:0.20547\n",
      "[611]\teval-logloss:0.23067\ttrain-logloss:0.20542\n",
      "[612]\teval-logloss:0.23060\ttrain-logloss:0.20530\n",
      "[613]\teval-logloss:0.23061\ttrain-logloss:0.20529\n",
      "[614]\teval-logloss:0.23056\ttrain-logloss:0.20525\n",
      "[615]\teval-logloss:0.23057\ttrain-logloss:0.20516\n",
      "[616]\teval-logloss:0.23052\ttrain-logloss:0.20510\n",
      "[617]\teval-logloss:0.23054\ttrain-logloss:0.20505\n",
      "[618]\teval-logloss:0.23054\ttrain-logloss:0.20501\n",
      "[619]\teval-logloss:0.23059\ttrain-logloss:0.20495\n",
      "[620]\teval-logloss:0.23062\ttrain-logloss:0.20485\n",
      "[621]\teval-logloss:0.23060\ttrain-logloss:0.20484\n",
      "[622]\teval-logloss:0.23062\ttrain-logloss:0.20480\n",
      "[623]\teval-logloss:0.23062\ttrain-logloss:0.20476\n",
      "[624]\teval-logloss:0.23053\ttrain-logloss:0.20473\n",
      "[625]\teval-logloss:0.23052\ttrain-logloss:0.20468\n",
      "[626]\teval-logloss:0.23050\ttrain-logloss:0.20465\n",
      "[627]\teval-logloss:0.23052\ttrain-logloss:0.20457\n",
      "[628]\teval-logloss:0.23048\ttrain-logloss:0.20449\n",
      "[629]\teval-logloss:0.23042\ttrain-logloss:0.20444\n",
      "[630]\teval-logloss:0.23042\ttrain-logloss:0.20439\n",
      "[631]\teval-logloss:0.23042\ttrain-logloss:0.20437\n",
      "[632]\teval-logloss:0.23044\ttrain-logloss:0.20430\n",
      "[633]\teval-logloss:0.23045\ttrain-logloss:0.20418\n",
      "[634]\teval-logloss:0.23043\ttrain-logloss:0.20417\n",
      "[635]\teval-logloss:0.23042\ttrain-logloss:0.20413\n",
      "[636]\teval-logloss:0.23043\ttrain-logloss:0.20411\n",
      "[637]\teval-logloss:0.23040\ttrain-logloss:0.20408\n",
      "[638]\teval-logloss:0.23041\ttrain-logloss:0.20402\n",
      "[639]\teval-logloss:0.23036\ttrain-logloss:0.20392\n",
      "[640]\teval-logloss:0.23038\ttrain-logloss:0.20385\n",
      "[641]\teval-logloss:0.23034\ttrain-logloss:0.20379\n",
      "[642]\teval-logloss:0.23037\ttrain-logloss:0.20372\n",
      "[643]\teval-logloss:0.23040\ttrain-logloss:0.20366\n",
      "[644]\teval-logloss:0.23040\ttrain-logloss:0.20365\n",
      "[645]\teval-logloss:0.23036\ttrain-logloss:0.20362\n",
      "[646]\teval-logloss:0.23032\ttrain-logloss:0.20359\n",
      "[647]\teval-logloss:0.23033\ttrain-logloss:0.20358\n",
      "[648]\teval-logloss:0.23032\ttrain-logloss:0.20354\n",
      "[649]\teval-logloss:0.23034\ttrain-logloss:0.20349\n",
      "[650]\teval-logloss:0.23029\ttrain-logloss:0.20348\n",
      "[651]\teval-logloss:0.23031\ttrain-logloss:0.20347\n",
      "[652]\teval-logloss:0.23033\ttrain-logloss:0.20341\n",
      "[653]\teval-logloss:0.23033\ttrain-logloss:0.20340\n",
      "[654]\teval-logloss:0.23035\ttrain-logloss:0.20340\n",
      "[655]\teval-logloss:0.23035\ttrain-logloss:0.20339\n",
      "[656]\teval-logloss:0.23036\ttrain-logloss:0.20337\n",
      "[657]\teval-logloss:0.23032\ttrain-logloss:0.20328\n",
      "[658]\teval-logloss:0.23031\ttrain-logloss:0.20322\n",
      "[659]\teval-logloss:0.23028\ttrain-logloss:0.20316\n",
      "[660]\teval-logloss:0.23035\ttrain-logloss:0.20310\n",
      "[661]\teval-logloss:0.23040\ttrain-logloss:0.20304\n",
      "[662]\teval-logloss:0.23039\ttrain-logloss:0.20302\n",
      "[663]\teval-logloss:0.23041\ttrain-logloss:0.20301\n",
      "[664]\teval-logloss:0.23043\ttrain-logloss:0.20298\n",
      "[665]\teval-logloss:0.23039\ttrain-logloss:0.20293\n",
      "[666]\teval-logloss:0.23040\ttrain-logloss:0.20289\n",
      "[667]\teval-logloss:0.23042\ttrain-logloss:0.20282\n",
      "[668]\teval-logloss:0.23040\ttrain-logloss:0.20279\n",
      "[669]\teval-logloss:0.23036\ttrain-logloss:0.20277\n",
      "[670]\teval-logloss:0.23034\ttrain-logloss:0.20272\n",
      "[671]\teval-logloss:0.23033\ttrain-logloss:0.20269\n",
      "[672]\teval-logloss:0.23032\ttrain-logloss:0.20265\n",
      "[673]\teval-logloss:0.23031\ttrain-logloss:0.20262\n",
      "[674]\teval-logloss:0.23031\ttrain-logloss:0.20256\n",
      "[675]\teval-logloss:0.23029\ttrain-logloss:0.20252\n",
      "[676]\teval-logloss:0.23024\ttrain-logloss:0.20245\n",
      "[677]\teval-logloss:0.23026\ttrain-logloss:0.20240\n",
      "[678]\teval-logloss:0.23026\ttrain-logloss:0.20237\n",
      "[679]\teval-logloss:0.23022\ttrain-logloss:0.20234\n",
      "[680]\teval-logloss:0.23019\ttrain-logloss:0.20226\n",
      "[681]\teval-logloss:0.23019\ttrain-logloss:0.20225\n",
      "[682]\teval-logloss:0.23015\ttrain-logloss:0.20221\n",
      "[683]\teval-logloss:0.23016\ttrain-logloss:0.20220\n",
      "[684]\teval-logloss:0.23016\ttrain-logloss:0.20216\n",
      "[685]\teval-logloss:0.23018\ttrain-logloss:0.20215\n",
      "[686]\teval-logloss:0.23016\ttrain-logloss:0.20211\n",
      "[687]\teval-logloss:0.23015\ttrain-logloss:0.20210\n",
      "[688]\teval-logloss:0.23015\ttrain-logloss:0.20209\n",
      "[689]\teval-logloss:0.23020\ttrain-logloss:0.20203\n",
      "[690]\teval-logloss:0.23022\ttrain-logloss:0.20202\n",
      "[691]\teval-logloss:0.23019\ttrain-logloss:0.20195\n",
      "[692]\teval-logloss:0.23020\ttrain-logloss:0.20191\n",
      "[693]\teval-logloss:0.23021\ttrain-logloss:0.20183\n",
      "[694]\teval-logloss:0.23019\ttrain-logloss:0.20177\n",
      "[695]\teval-logloss:0.23019\ttrain-logloss:0.20172\n",
      "[696]\teval-logloss:0.23021\ttrain-logloss:0.20172\n",
      "[697]\teval-logloss:0.23018\ttrain-logloss:0.20167\n",
      "[698]\teval-logloss:0.23023\ttrain-logloss:0.20163\n",
      "[699]\teval-logloss:0.23021\ttrain-logloss:0.20162\n",
      "[700]\teval-logloss:0.23014\ttrain-logloss:0.20157\n",
      "[701]\teval-logloss:0.23016\ttrain-logloss:0.20156\n",
      "[702]\teval-logloss:0.23016\ttrain-logloss:0.20153\n",
      "[703]\teval-logloss:0.23015\ttrain-logloss:0.20149\n",
      "[704]\teval-logloss:0.23013\ttrain-logloss:0.20141\n",
      "[705]\teval-logloss:0.23013\ttrain-logloss:0.20140\n",
      "[706]\teval-logloss:0.23015\ttrain-logloss:0.20139\n",
      "[707]\teval-logloss:0.23014\ttrain-logloss:0.20137\n",
      "[708]\teval-logloss:0.23010\ttrain-logloss:0.20135\n",
      "[709]\teval-logloss:0.23009\ttrain-logloss:0.20134\n",
      "[710]\teval-logloss:0.23010\ttrain-logloss:0.20133\n",
      "[711]\teval-logloss:0.23007\ttrain-logloss:0.20132\n",
      "[712]\teval-logloss:0.23008\ttrain-logloss:0.20131\n",
      "[713]\teval-logloss:0.23002\ttrain-logloss:0.20123\n",
      "[714]\teval-logloss:0.22996\ttrain-logloss:0.20116\n",
      "[715]\teval-logloss:0.22996\ttrain-logloss:0.20112\n",
      "[716]\teval-logloss:0.22995\ttrain-logloss:0.20110\n",
      "[717]\teval-logloss:0.22994\ttrain-logloss:0.20107\n",
      "[718]\teval-logloss:0.22993\ttrain-logloss:0.20105\n",
      "[719]\teval-logloss:0.22993\ttrain-logloss:0.20104\n",
      "[720]\teval-logloss:0.22993\ttrain-logloss:0.20100\n",
      "[721]\teval-logloss:0.22994\ttrain-logloss:0.20095\n",
      "[722]\teval-logloss:0.22990\ttrain-logloss:0.20092\n",
      "[723]\teval-logloss:0.22989\ttrain-logloss:0.20092\n",
      "[724]\teval-logloss:0.22990\ttrain-logloss:0.20091\n",
      "[725]\teval-logloss:0.22989\ttrain-logloss:0.20087\n",
      "[726]\teval-logloss:0.22993\ttrain-logloss:0.20084\n",
      "[727]\teval-logloss:0.22998\ttrain-logloss:0.20079\n",
      "[728]\teval-logloss:0.23000\ttrain-logloss:0.20072\n",
      "[729]\teval-logloss:0.23003\ttrain-logloss:0.20069\n",
      "[730]\teval-logloss:0.23003\ttrain-logloss:0.20063\n",
      "[731]\teval-logloss:0.23000\ttrain-logloss:0.20059\n",
      "[732]\teval-logloss:0.23002\ttrain-logloss:0.20059\n",
      "[733]\teval-logloss:0.23009\ttrain-logloss:0.20056\n",
      "[734]\teval-logloss:0.23009\ttrain-logloss:0.20053\n",
      "[735]\teval-logloss:0.23005\ttrain-logloss:0.20046\n",
      "[736]\teval-logloss:0.23007\ttrain-logloss:0.20045\n",
      "[737]\teval-logloss:0.23006\ttrain-logloss:0.20043\n",
      "[738]\teval-logloss:0.23008\ttrain-logloss:0.20036\n",
      "[739]\teval-logloss:0.23011\ttrain-logloss:0.20033\n",
      "[740]\teval-logloss:0.23008\ttrain-logloss:0.20029\n",
      "[741]\teval-logloss:0.23006\ttrain-logloss:0.20015\n",
      "[742]\teval-logloss:0.23008\ttrain-logloss:0.20009\n",
      "[743]\teval-logloss:0.23014\ttrain-logloss:0.20004\n",
      "[744]\teval-logloss:0.23011\ttrain-logloss:0.20002\n",
      "[745]\teval-logloss:0.23012\ttrain-logloss:0.19999\n",
      "[746]\teval-logloss:0.23010\ttrain-logloss:0.19994\n",
      "[747]\teval-logloss:0.23007\ttrain-logloss:0.19993\n",
      "[748]\teval-logloss:0.23008\ttrain-logloss:0.19989\n",
      "[749]\teval-logloss:0.23008\ttrain-logloss:0.19988\n",
      "[750]\teval-logloss:0.23006\ttrain-logloss:0.19987\n",
      "[751]\teval-logloss:0.23006\ttrain-logloss:0.19982\n",
      "[752]\teval-logloss:0.23001\ttrain-logloss:0.19979\n",
      "[753]\teval-logloss:0.23003\ttrain-logloss:0.19974\n",
      "[754]\teval-logloss:0.23003\ttrain-logloss:0.19972\n",
      "[755]\teval-logloss:0.23004\ttrain-logloss:0.19969\n",
      "[756]\teval-logloss:0.23003\ttrain-logloss:0.19968\n",
      "[757]\teval-logloss:0.23003\ttrain-logloss:0.19967\n",
      "[758]\teval-logloss:0.23006\ttrain-logloss:0.19960\n",
      "[759]\teval-logloss:0.23005\ttrain-logloss:0.19954\n",
      "[760]\teval-logloss:0.23000\ttrain-logloss:0.19949\n",
      "[761]\teval-logloss:0.22991\ttrain-logloss:0.19946\n",
      "[762]\teval-logloss:0.22990\ttrain-logloss:0.19940\n",
      "[763]\teval-logloss:0.22991\ttrain-logloss:0.19939\n",
      "[764]\teval-logloss:0.22997\ttrain-logloss:0.19934\n",
      "[765]\teval-logloss:0.22994\ttrain-logloss:0.19925\n",
      "[766]\teval-logloss:0.22990\ttrain-logloss:0.19921\n",
      "[767]\teval-logloss:0.22986\ttrain-logloss:0.19914\n",
      "[768]\teval-logloss:0.22987\ttrain-logloss:0.19909\n",
      "[769]\teval-logloss:0.22984\ttrain-logloss:0.19906\n",
      "[770]\teval-logloss:0.22985\ttrain-logloss:0.19903\n",
      "[771]\teval-logloss:0.22986\ttrain-logloss:0.19901\n",
      "[772]\teval-logloss:0.22986\ttrain-logloss:0.19900\n",
      "[773]\teval-logloss:0.22987\ttrain-logloss:0.19895\n",
      "[774]\teval-logloss:0.22982\ttrain-logloss:0.19891\n",
      "[775]\teval-logloss:0.22979\ttrain-logloss:0.19886\n",
      "[776]\teval-logloss:0.22982\ttrain-logloss:0.19882\n",
      "[777]\teval-logloss:0.22984\ttrain-logloss:0.19879\n",
      "[778]\teval-logloss:0.22982\ttrain-logloss:0.19872\n",
      "[779]\teval-logloss:0.22982\ttrain-logloss:0.19869\n",
      "[780]\teval-logloss:0.22977\ttrain-logloss:0.19866\n",
      "[781]\teval-logloss:0.22979\ttrain-logloss:0.19862\n",
      "[782]\teval-logloss:0.22980\ttrain-logloss:0.19861\n",
      "[783]\teval-logloss:0.22977\ttrain-logloss:0.19856\n",
      "[784]\teval-logloss:0.22979\ttrain-logloss:0.19855\n",
      "[785]\teval-logloss:0.22978\ttrain-logloss:0.19854\n",
      "[786]\teval-logloss:0.22979\ttrain-logloss:0.19850\n",
      "[787]\teval-logloss:0.22979\ttrain-logloss:0.19843\n",
      "[788]\teval-logloss:0.22973\ttrain-logloss:0.19841\n",
      "[789]\teval-logloss:0.22975\ttrain-logloss:0.19839\n",
      "[790]\teval-logloss:0.22976\ttrain-logloss:0.19835\n",
      "[791]\teval-logloss:0.22981\ttrain-logloss:0.19827\n",
      "[792]\teval-logloss:0.22980\ttrain-logloss:0.19826\n",
      "[793]\teval-logloss:0.22983\ttrain-logloss:0.19825\n",
      "[794]\teval-logloss:0.22984\ttrain-logloss:0.19824\n",
      "[795]\teval-logloss:0.22976\ttrain-logloss:0.19818\n",
      "[796]\teval-logloss:0.22975\ttrain-logloss:0.19811\n",
      "[797]\teval-logloss:0.22974\ttrain-logloss:0.19810\n",
      "[798]\teval-logloss:0.22970\ttrain-logloss:0.19808\n",
      "[799]\teval-logloss:0.22970\ttrain-logloss:0.19806\n",
      "[800]\teval-logloss:0.22974\ttrain-logloss:0.19803\n",
      "[801]\teval-logloss:0.22971\ttrain-logloss:0.19796\n",
      "[802]\teval-logloss:0.22973\ttrain-logloss:0.19792\n",
      "[803]\teval-logloss:0.22976\ttrain-logloss:0.19788\n",
      "[804]\teval-logloss:0.22979\ttrain-logloss:0.19783\n",
      "[805]\teval-logloss:0.22979\ttrain-logloss:0.19782\n",
      "[806]\teval-logloss:0.22975\ttrain-logloss:0.19778\n",
      "[807]\teval-logloss:0.22972\ttrain-logloss:0.19777\n",
      "[808]\teval-logloss:0.22969\ttrain-logloss:0.19770\n",
      "[809]\teval-logloss:0.22970\ttrain-logloss:0.19767\n",
      "[810]\teval-logloss:0.22971\ttrain-logloss:0.19766\n",
      "[811]\teval-logloss:0.22969\ttrain-logloss:0.19765\n",
      "[812]\teval-logloss:0.22971\ttrain-logloss:0.19763\n",
      "[813]\teval-logloss:0.22971\ttrain-logloss:0.19762\n",
      "[814]\teval-logloss:0.22970\ttrain-logloss:0.19758\n",
      "[815]\teval-logloss:0.22965\ttrain-logloss:0.19752\n",
      "[816]\teval-logloss:0.22964\ttrain-logloss:0.19746\n",
      "[817]\teval-logloss:0.22965\ttrain-logloss:0.19745\n",
      "[818]\teval-logloss:0.22962\ttrain-logloss:0.19743\n",
      "[819]\teval-logloss:0.22965\ttrain-logloss:0.19736\n",
      "[820]\teval-logloss:0.22966\ttrain-logloss:0.19732\n",
      "[821]\teval-logloss:0.22967\ttrain-logloss:0.19730\n",
      "[822]\teval-logloss:0.22965\ttrain-logloss:0.19727\n",
      "[823]\teval-logloss:0.22967\ttrain-logloss:0.19726\n",
      "[824]\teval-logloss:0.22972\ttrain-logloss:0.19724\n",
      "[825]\teval-logloss:0.22970\ttrain-logloss:0.19718\n",
      "[826]\teval-logloss:0.22970\ttrain-logloss:0.19716\n",
      "[827]\teval-logloss:0.22970\ttrain-logloss:0.19715\n",
      "[828]\teval-logloss:0.22971\ttrain-logloss:0.19714\n",
      "[829]\teval-logloss:0.22965\ttrain-logloss:0.19703\n",
      "[830]\teval-logloss:0.22970\ttrain-logloss:0.19698\n",
      "[831]\teval-logloss:0.22971\ttrain-logloss:0.19695\n",
      "[832]\teval-logloss:0.22974\ttrain-logloss:0.19694\n",
      "[833]\teval-logloss:0.22975\ttrain-logloss:0.19687\n",
      "[834]\teval-logloss:0.22976\ttrain-logloss:0.19685\n",
      "[835]\teval-logloss:0.22972\ttrain-logloss:0.19681\n",
      "[836]\teval-logloss:0.22974\ttrain-logloss:0.19673\n",
      "[837]\teval-logloss:0.22977\ttrain-logloss:0.19668\n",
      "[838]\teval-logloss:0.22970\ttrain-logloss:0.19662\n",
      "[839]\teval-logloss:0.22967\ttrain-logloss:0.19657\n",
      "[840]\teval-logloss:0.22965\ttrain-logloss:0.19653\n",
      "[841]\teval-logloss:0.22966\ttrain-logloss:0.19650\n",
      "[842]\teval-logloss:0.22966\ttrain-logloss:0.19647\n",
      "[843]\teval-logloss:0.22967\ttrain-logloss:0.19647\n",
      "[844]\teval-logloss:0.22970\ttrain-logloss:0.19646\n",
      "[845]\teval-logloss:0.22965\ttrain-logloss:0.19642\n",
      "[846]\teval-logloss:0.22960\ttrain-logloss:0.19638\n",
      "[847]\teval-logloss:0.22961\ttrain-logloss:0.19636\n",
      "[848]\teval-logloss:0.22960\ttrain-logloss:0.19634\n",
      "[849]\teval-logloss:0.22964\ttrain-logloss:0.19631\n",
      "[850]\teval-logloss:0.22960\ttrain-logloss:0.19625\n",
      "[851]\teval-logloss:0.22959\ttrain-logloss:0.19624\n",
      "[852]\teval-logloss:0.22960\ttrain-logloss:0.19620\n",
      "[853]\teval-logloss:0.22956\ttrain-logloss:0.19613\n",
      "[854]\teval-logloss:0.22958\ttrain-logloss:0.19608\n",
      "[855]\teval-logloss:0.22960\ttrain-logloss:0.19607\n",
      "[856]\teval-logloss:0.22952\ttrain-logloss:0.19602\n",
      "[857]\teval-logloss:0.22950\ttrain-logloss:0.19601\n",
      "[858]\teval-logloss:0.22951\ttrain-logloss:0.19601\n",
      "[859]\teval-logloss:0.22950\ttrain-logloss:0.19594\n",
      "[860]\teval-logloss:0.22951\ttrain-logloss:0.19591\n",
      "[861]\teval-logloss:0.22950\ttrain-logloss:0.19590\n",
      "[862]\teval-logloss:0.22949\ttrain-logloss:0.19590\n",
      "[863]\teval-logloss:0.22951\ttrain-logloss:0.19588\n",
      "[864]\teval-logloss:0.22951\ttrain-logloss:0.19586\n",
      "[865]\teval-logloss:0.22952\ttrain-logloss:0.19585\n",
      "[866]\teval-logloss:0.22954\ttrain-logloss:0.19585\n",
      "[867]\teval-logloss:0.22955\ttrain-logloss:0.19584\n",
      "[868]\teval-logloss:0.22957\ttrain-logloss:0.19583\n",
      "[869]\teval-logloss:0.22958\ttrain-logloss:0.19582\n",
      "[870]\teval-logloss:0.22963\ttrain-logloss:0.19579\n",
      "[871]\teval-logloss:0.22958\ttrain-logloss:0.19574\n",
      "[872]\teval-logloss:0.22958\ttrain-logloss:0.19568\n",
      "[873]\teval-logloss:0.22948\ttrain-logloss:0.19563\n",
      "[874]\teval-logloss:0.22950\ttrain-logloss:0.19562\n",
      "[875]\teval-logloss:0.22954\ttrain-logloss:0.19561\n",
      "[876]\teval-logloss:0.22956\ttrain-logloss:0.19558\n",
      "[877]\teval-logloss:0.22960\ttrain-logloss:0.19557\n",
      "[878]\teval-logloss:0.22958\ttrain-logloss:0.19553\n",
      "[879]\teval-logloss:0.22956\ttrain-logloss:0.19549\n",
      "[880]\teval-logloss:0.22956\ttrain-logloss:0.19549\n",
      "[881]\teval-logloss:0.22954\ttrain-logloss:0.19544\n",
      "[882]\teval-logloss:0.22956\ttrain-logloss:0.19541\n",
      "[883]\teval-logloss:0.22957\ttrain-logloss:0.19540\n",
      "[884]\teval-logloss:0.22963\ttrain-logloss:0.19533\n",
      "[885]\teval-logloss:0.22966\ttrain-logloss:0.19530\n",
      "[886]\teval-logloss:0.22965\ttrain-logloss:0.19524\n",
      "[887]\teval-logloss:0.22964\ttrain-logloss:0.19522\n",
      "[888]\teval-logloss:0.22970\ttrain-logloss:0.19516\n",
      "[889]\teval-logloss:0.22965\ttrain-logloss:0.19511\n",
      "[890]\teval-logloss:0.22968\ttrain-logloss:0.19509\n",
      "[891]\teval-logloss:0.22972\ttrain-logloss:0.19505\n",
      "[892]\teval-logloss:0.22967\ttrain-logloss:0.19499\n",
      "[893]\teval-logloss:0.22968\ttrain-logloss:0.19495\n",
      "[894]\teval-logloss:0.22972\ttrain-logloss:0.19490\n",
      "[895]\teval-logloss:0.22972\ttrain-logloss:0.19487\n",
      "[896]\teval-logloss:0.22972\ttrain-logloss:0.19485\n",
      "[897]\teval-logloss:0.22970\ttrain-logloss:0.19477\n",
      "[898]\teval-logloss:0.22972\ttrain-logloss:0.19472\n",
      "[899]\teval-logloss:0.22970\ttrain-logloss:0.19469\n",
      "[900]\teval-logloss:0.22970\ttrain-logloss:0.19464\n",
      "[901]\teval-logloss:0.22969\ttrain-logloss:0.19463\n",
      "[902]\teval-logloss:0.22966\ttrain-logloss:0.19459\n",
      "[903]\teval-logloss:0.22961\ttrain-logloss:0.19454\n",
      "[904]\teval-logloss:0.22959\ttrain-logloss:0.19450\n",
      "[905]\teval-logloss:0.22962\ttrain-logloss:0.19449\n",
      "[906]\teval-logloss:0.22964\ttrain-logloss:0.19444\n",
      "[907]\teval-logloss:0.22959\ttrain-logloss:0.19441\n",
      "[908]\teval-logloss:0.22960\ttrain-logloss:0.19440\n",
      "[909]\teval-logloss:0.22959\ttrain-logloss:0.19439\n",
      "[910]\teval-logloss:0.22963\ttrain-logloss:0.19438\n",
      "[911]\teval-logloss:0.22957\ttrain-logloss:0.19429\n",
      "[912]\teval-logloss:0.22958\ttrain-logloss:0.19423\n",
      "[913]\teval-logloss:0.22960\ttrain-logloss:0.19421\n",
      "[914]\teval-logloss:0.22958\ttrain-logloss:0.19416\n",
      "[915]\teval-logloss:0.22955\ttrain-logloss:0.19412\n",
      "[916]\teval-logloss:0.22959\ttrain-logloss:0.19408\n",
      "[917]\teval-logloss:0.22959\ttrain-logloss:0.19408\n",
      "[918]\teval-logloss:0.22956\ttrain-logloss:0.19406\n",
      "[919]\teval-logloss:0.22955\ttrain-logloss:0.19402\n",
      "[920]\teval-logloss:0.22954\ttrain-logloss:0.19401\n",
      "[921]\teval-logloss:0.22953\ttrain-logloss:0.19401\n",
      "[922]\teval-logloss:0.22948\ttrain-logloss:0.19393\n",
      "[923]\teval-logloss:0.22950\ttrain-logloss:0.19389\n",
      "[924]\teval-logloss:0.22950\ttrain-logloss:0.19388\n",
      "[925]\teval-logloss:0.22954\ttrain-logloss:0.19384\n",
      "[926]\teval-logloss:0.22958\ttrain-logloss:0.19381\n",
      "[927]\teval-logloss:0.22957\ttrain-logloss:0.19380\n",
      "[928]\teval-logloss:0.22961\ttrain-logloss:0.19379\n",
      "[929]\teval-logloss:0.22961\ttrain-logloss:0.19377\n",
      "[930]\teval-logloss:0.22961\ttrain-logloss:0.19373\n",
      "[931]\teval-logloss:0.22965\ttrain-logloss:0.19369\n",
      "[932]\teval-logloss:0.22966\ttrain-logloss:0.19363\n",
      "[933]\teval-logloss:0.22966\ttrain-logloss:0.19360\n",
      "[934]\teval-logloss:0.22962\ttrain-logloss:0.19357\n",
      "[935]\teval-logloss:0.22961\ttrain-logloss:0.19352\n",
      "[936]\teval-logloss:0.22961\ttrain-logloss:0.19352\n",
      "[937]\teval-logloss:0.22963\ttrain-logloss:0.19349\n",
      "[938]\teval-logloss:0.22958\ttrain-logloss:0.19345\n",
      "[939]\teval-logloss:0.22959\ttrain-logloss:0.19339\n",
      "[940]\teval-logloss:0.22957\ttrain-logloss:0.19336\n",
      "[941]\teval-logloss:0.22954\ttrain-logloss:0.19330\n",
      "[942]\teval-logloss:0.22954\ttrain-logloss:0.19328\n",
      "[943]\teval-logloss:0.22951\ttrain-logloss:0.19327\n",
      "[944]\teval-logloss:0.22951\ttrain-logloss:0.19327\n",
      "[945]\teval-logloss:0.22950\ttrain-logloss:0.19322\n",
      "[946]\teval-logloss:0.22951\ttrain-logloss:0.19318\n",
      "[947]\teval-logloss:0.22952\ttrain-logloss:0.19318\n",
      "[948]\teval-logloss:0.22956\ttrain-logloss:0.19312\n",
      "[949]\teval-logloss:0.22957\ttrain-logloss:0.19307\n",
      "[950]\teval-logloss:0.22954\ttrain-logloss:0.19304\n",
      "[951]\teval-logloss:0.22957\ttrain-logloss:0.19299\n",
      "[952]\teval-logloss:0.22957\ttrain-logloss:0.19297\n",
      "[953]\teval-logloss:0.22956\ttrain-logloss:0.19296\n",
      "[954]\teval-logloss:0.22958\ttrain-logloss:0.19296\n",
      "[955]\teval-logloss:0.22957\ttrain-logloss:0.19295\n",
      "[956]\teval-logloss:0.22959\ttrain-logloss:0.19293\n",
      "[957]\teval-logloss:0.22957\ttrain-logloss:0.19289\n",
      "[958]\teval-logloss:0.22957\ttrain-logloss:0.19283\n",
      "[959]\teval-logloss:0.22956\ttrain-logloss:0.19281\n",
      "[960]\teval-logloss:0.22951\ttrain-logloss:0.19276\n",
      "[961]\teval-logloss:0.22948\ttrain-logloss:0.19273\n",
      "[962]\teval-logloss:0.22952\ttrain-logloss:0.19268\n",
      "[963]\teval-logloss:0.22954\ttrain-logloss:0.19267\n",
      "[964]\teval-logloss:0.22953\ttrain-logloss:0.19265\n",
      "[965]\teval-logloss:0.22953\ttrain-logloss:0.19262\n",
      "[966]\teval-logloss:0.22957\ttrain-logloss:0.19258\n",
      "[967]\teval-logloss:0.22959\ttrain-logloss:0.19254\n",
      "[968]\teval-logloss:0.22960\ttrain-logloss:0.19252\n",
      "[969]\teval-logloss:0.22964\ttrain-logloss:0.19249\n",
      "[970]\teval-logloss:0.22966\ttrain-logloss:0.19245\n",
      "[971]\teval-logloss:0.22969\ttrain-logloss:0.19242\n",
      "[972]\teval-logloss:0.22971\ttrain-logloss:0.19240\n",
      "[973]\teval-logloss:0.22977\ttrain-logloss:0.19235\n",
      "[974]\teval-logloss:0.22977\ttrain-logloss:0.19229\n",
      "[975]\teval-logloss:0.22976\ttrain-logloss:0.19228\n",
      "[976]\teval-logloss:0.22977\ttrain-logloss:0.19224\n",
      "[977]\teval-logloss:0.22975\ttrain-logloss:0.19219\n",
      "[978]\teval-logloss:0.22977\ttrain-logloss:0.19215\n",
      "[979]\teval-logloss:0.22976\ttrain-logloss:0.19215\n",
      "[980]\teval-logloss:0.22980\ttrain-logloss:0.19214\n",
      "[981]\teval-logloss:0.22983\ttrain-logloss:0.19208\n",
      "[982]\teval-logloss:0.22983\ttrain-logloss:0.19207\n",
      "[983]\teval-logloss:0.22980\ttrain-logloss:0.19202\n",
      "[984]\teval-logloss:0.22985\ttrain-logloss:0.19200\n",
      "[985]\teval-logloss:0.22987\ttrain-logloss:0.19196\n",
      "[986]\teval-logloss:0.22989\ttrain-logloss:0.19194\n",
      "[987]\teval-logloss:0.22990\ttrain-logloss:0.19191\n",
      "[988]\teval-logloss:0.22992\ttrain-logloss:0.19185\n",
      "[989]\teval-logloss:0.22989\ttrain-logloss:0.19182\n",
      "[990]\teval-logloss:0.22981\ttrain-logloss:0.19175\n",
      "[991]\teval-logloss:0.22980\ttrain-logloss:0.19173\n",
      "[992]\teval-logloss:0.22983\ttrain-logloss:0.19168\n",
      "[993]\teval-logloss:0.22986\ttrain-logloss:0.19166\n",
      "[994]\teval-logloss:0.22984\ttrain-logloss:0.19163\n",
      "[995]\teval-logloss:0.22979\ttrain-logloss:0.19160\n",
      "[996]\teval-logloss:0.22979\ttrain-logloss:0.19157\n",
      "[997]\teval-logloss:0.22981\ttrain-logloss:0.19157\n",
      "[998]\teval-logloss:0.22979\ttrain-logloss:0.19155\n",
      "[999]\teval-logloss:0.22982\ttrain-logloss:0.19150\n",
      "Accuracy: 0.9115539241391365\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95      4809\n",
      "           1       0.86      0.54      0.67       912\n",
      "\n",
      "    accuracy                           0.91      5721\n",
      "   macro avg       0.89      0.76      0.81      5721\n",
      "weighted avg       0.91      0.91      0.90      5721\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4718,   91],\n",
       "       [ 415,  497]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'objective': 'binary:logistic',  # для задачи бинарной классификации\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 1000,\n",
    "    'eval_metric': 'logloss'\n",
    "}\n",
    "\n",
    "# Создаем объект DMatrix для данных\n",
    "train_data = xgb.DMatrix(X_train_std, label=y_train)\n",
    "test_data = xgb.DMatrix(X_test_std, label=y_test)\n",
    "\n",
    "# Обучаем модель с ранней остановкой (early stopping)\n",
    "watchlist = [(test_data, 'eval'), (train_data, 'train')]\n",
    "num_round = 1000  # большое количество итераций\n",
    "bst = xgb.train(params, train_data, num_round, evals=watchlist, early_stopping_rounds=10)\n",
    "\n",
    "# Предсказываем на тестовом наборе данных\n",
    "y_pred = bst.predict(test_data)\n",
    "\n",
    "# Оцениваем качество модели\n",
    "y_pred_binary = np.round(y_pred)  # преобразуем вероятности в бинарные предсказания\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(report)\n",
    "confusion_matrix(y_test, y_pred_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[y_test == 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_pred == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Важность признака Семестр равна -0.956222860367033\n",
      "Важность признака Форма обучения равна 0.1598202852549684\n",
      "Важность признака Квалификация равна -0.29194072457118514\n",
      "Важность признака незачет сразу равна 0.0\n",
      "Важность признака зачет сразу равна -0.15757901100135419\n",
      "Важность признака удовлетворительно сразу равна 0.08299480598579387\n",
      "Важность признака хорошо сразу равна -0.20043796394497296\n",
      "Важность признака отлично сразу равна -0.40053436044570895\n",
      "Важность признака зачет с исправлением равна 0.308135208540496\n",
      "Важность признака удовлетворительно с исправлением равна -0.20779939075103881\n",
      "Важность признака хорошо с исправлением равна -0.0123359071861901\n",
      "Важность признака отлично с исправлением равна -0.3164868078049016\n",
      "Важность признака незачет до исправления равна 0.13711429429267727\n",
      "Важность признака зачет до исправления равна -0.17437642924232838\n",
      "Важность признака удовлетворительно до исправления равна -0.022385418060579955\n",
      "Важность признака хорошо до исправления равна -0.16883934419133415\n",
      "Важность признака Накоп незачет сразу равна 6.952933340429399\n",
      "Важность признака Накоп зачет сразу равна -0.004031440480620843\n",
      "Важность признака Накоп удовлетворительно сразу равна 0.14174240566231033\n",
      "Важность признака Накоп хорошо сразу равна 0.17707326904867987\n",
      "Важность признака Накоп отлично сразу равна 0.033400840755299865\n",
      "Важность признака Накоп зачет с исправлением равна -0.5392828971603857\n",
      "Важность признака Накоп удовлетворительно с исправлением равна -0.2028182581329736\n",
      "Важность признака Накоп хорошо с исправлением равна 0.08345908636564793\n",
      "Важность признака Накоп отлично с исправлением равна -0.06953759584991592\n",
      "Важность признака Накоп незачет до исправления равна 0.5375854073572709\n",
      "Важность признака Накоп зачет до исправления равна 0.3384737239750576\n",
      "Важность признака Накоп удовлетворительно до исправления равна -0.39259592726951315\n"
     ]
    }
   ],
   "source": [
    "for column, coef in zip(data.columns, lr.coef_[0]):\n",
    "    print(f'Важность признака {column} равна {coef}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.49852229e-01,  1.08975607e-01, -2.72577701e-01,  0.00000000e+00,\n",
       "       -1.76052968e-01,  1.08859017e-01, -1.81742184e-01, -3.82745231e-01,\n",
       "        1.66923713e-01, -1.07898967e-01,  7.39549741e-02, -3.42514923e-01,\n",
       "        6.39133728e-02, -2.60078462e-01, -1.82990766e-04, -1.31871222e-02,\n",
       "        6.46200397e+00, -1.49514477e-02,  1.38186829e-01,  1.72984895e-01,\n",
       "        2.97954693e-02, -6.42259076e-01, -4.95371751e-01, -2.99388069e-01,\n",
       "       -2.83218758e-01,  7.09152083e-01,  6.05729588e-01, -8.11245553e-02])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
